{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, re\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFiles (path):\n",
    "    files = sorted(glob.glob(path+'*.txt'))\n",
    "    \n",
    "    data=[]\n",
    "    topic=[]\n",
    "    Business=[]\n",
    "    Health=[]\n",
    "    Showbiz=[]\n",
    "    Sports=[]\n",
    "    Technology=[]\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "         with open(file_path,'r') as infile:\n",
    "            text=infile.read()\n",
    "            data.append(text)\n",
    "            file_topic=''.join(re.findall('[A-Za-z]',file_path.split('/')[3].split('.')[0]))\n",
    "            if file_topic == 'bus':\n",
    "                Business.append(text)\n",
    "            elif file_topic == 'hlth':\n",
    "                Health.append(text)\n",
    "            elif file_topic == 'sbz':\n",
    "                Showbiz.append(text)\n",
    "            elif file_topic == 'sp':\n",
    "                Sports.append(text)\n",
    "            else:\n",
    "                Technology.append(text)\n",
    "                \n",
    "    return Business, Health, Showbiz, Sports, Technology, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diccionary(ruta):\n",
    "    terms = set()#Dictionary of slangs\n",
    "    try:\n",
    "        tmp = open(ruta, \"r\")     \n",
    "        while True :\n",
    "            linea = tmp.readline()                                                                                   \n",
    "            #linea = to_unicode(linea) \n",
    "            if (not linea) or (linea == \"\"):                                                                               \n",
    "                break;                                                                                                      \n",
    "            linea = linea.rstrip()\n",
    "            terms.add(linea.lower())\n",
    "        return (terms)\n",
    "    except IOError as e:\n",
    "        print (\"Error: \"+ruta+\" I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(dataset):\n",
    "    lista=[]\n",
    "    for documento in dataset:\n",
    "        doc=documento.lower()\n",
    "        tokens=doc.split()\n",
    "        lista.append(tokens)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords (dataset):\n",
    "    stop_words = load_diccionary('stop_words.txt')\n",
    "    lista=[]\n",
    "    for doc in dataset:\n",
    "        aux=[]\n",
    "        for word in doc: \n",
    "            if word not in stop_words:\n",
    "                aux.append(word)\n",
    "        lista.append(aux)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary (dataset):\n",
    "    data=tokenization(dataset)\n",
    "    #data=remove_stopwords(data)\n",
    "    aux=[]\n",
    "    for docx in data:\n",
    "        aux.extend(docx)\n",
    "    resultado=set(aux)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(docsX, docsY):\n",
    "    \n",
    "    tamX=len(docsX)\n",
    "    tamY=len(docsY)\n",
    "    \n",
    "    inter=docsX.intersection(docsY)\n",
    "    tamC=len(inter)\n",
    "    \n",
    "    tamTotal=tamX+tamY\n",
    "    \n",
    "    interX=tamC/tamTotal\n",
    "    interY=tamC/tamTotal\n",
    "    \n",
    "    \n",
    "    print (\"Real: \"+str(tamX))\n",
    "    print (\"Fake: \"+str(tamY))\n",
    "    print (\"Intersection: \"+str(tamC))\n",
    "    print (\"Intersection (%): \"+str(interX*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: 70\n",
      "Fake: 36\n",
      "Real: 4640\n",
      "Fake: 1939\n",
      "Intersection: 1693\n",
      "Intersection (%): 25.733394132846936\n"
     ]
    }
   ],
   "source": [
    "#Training Statistics\n",
    "    \n",
    "path_trueTrain= 'Corpus/Train/Real/'\n",
    "path_fakeTrain= \"Corpus/Train/Fake/\"\n",
    "\n",
    "BusinessT, HealthT, ShowbizT, SportsT, TechnologyT, dataT = openFiles(path_trueTrain)\n",
    "BusinessF, HealthF, ShowbizF, SportsF, TechnologyF, dataF = openFiles(path_fakeTrain)\n",
    "\n",
    "print ('Real:',len(BusinessT))\n",
    "print ('Fake:',len(BusinessF))\n",
    "\n",
    "#_____________________________________________________________________\n",
    "\n",
    "set_1=BusinessT\n",
    "set_2=BusinessF\n",
    "\n",
    "#set_1=pd.concat([BusinessT,BusinessF], axis=0)\n",
    "#set_2=pd.concat([HealthT,HealthF],axis=0)\n",
    "\n",
    "voc_set1=vocabulary(set_1)\n",
    "voc_set2=vocabulary(set_2)\n",
    "\n",
    "estad=comparison(voc_set1,voc_set2)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: 30\n",
      "Fake: 14\n",
      "Real: 2822\n",
      "Fake: 862\n",
      "Intersection: 753\n",
      "Intersection (%): 20.439739413680783\n"
     ]
    }
   ],
   "source": [
    "# Testing Statistics\n",
    "path_trueTest = 'Corpus/Test/Real/'\n",
    "path_fakeTest = \"Corpus/Test/Fake/\"\n",
    "\n",
    "BusinessT, HealthT, ShowbizT, SportsT, TechnologyT, dataT = openFiles(path_trueTest)\n",
    "BusinessF, HealthF, ShowbizF, SportsF, TechnologyF, dataF = openFiles(path_fakeTest)\n",
    "\n",
    "print ('Real:',len(BusinessT))\n",
    "print ('Fake:',len(BusinessF))\n",
    "\n",
    "#_____________________________________________________________________\n",
    "\n",
    "set_1=BusinessT\n",
    "set_2=BusinessF\n",
    "\n",
    "#set_1=pd.concat([BusinessT,BusinessF], axis=0)\n",
    "#set_2=pd.concat([HealthT,HealthF],axis=0)\n",
    "\n",
    "voc_set1=vocabulary(set_1)\n",
    "voc_set2=vocabulary(set_2)\n",
    "\n",
    "estad=comparison(voc_set1,voc_set2)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
