{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fake-news-detection\" data-toc-modified-id=\"Fake-news-detection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fake news detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-extraction-functions\" data-toc-modified-id=\"Feature-extraction-functions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Feature extraction functions</a></span></li><li><span><a href=\"#Reading-and-preparing-the-corpus\" data-toc-modified-id=\"Reading-and-preparing-the-corpus-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Reading and preparing the corpus</a></span></li><li><span><a href=\"#Parametrization-and-feature-extraction\" data-toc-modified-id=\"Parametrization-and-feature-extraction-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Parametrization and feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generating-experiment-sets\" data-toc-modified-id=\"Generating-experiment-sets-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Generating experiment sets</a></span></li></ul></li><li><span><a href=\"#Frequency-threshold\" data-toc-modified-id=\"Frequency-threshold-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Frequency threshold</a></span></li><li><span><a href=\"#Weighting-schemes\" data-toc-modified-id=\"Weighting-schemes-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Weighting schemes</a></span></li><li><span><a href=\"#Classification-Process---Training\" data-toc-modified-id=\"Classification-Process---Training-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Classification Process - Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initializing-classification-algorithms\" data-toc-modified-id=\"Initializing-classification-algorithms-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Initializing classification algorithms</a></span></li></ul></li><li><span><a href=\"#End-to-End-Pipeline\" data-toc-modified-id=\"End-to-End-Pipeline-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>End-to-End Pipeline</a></span></li><li><span><a href=\"#Classification-Process---Testing\" data-toc-modified-id=\"Classification-Process---Testing-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Classification Process - Testing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to make it run on Alisa's laptop \n",
    "import sys \n",
    "#sys.executable, sys.path\n",
    "sys.path.append('/Users/alisa/workspace/Urdu_Fake_News/envname/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import codecs\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from random import randrange\n",
    "from scipy.sparse import csr_matrix, csc_matrix, hstack, coo_matrix\n",
    "from gensim.matutils import Scipy2Corpus, corpus2csc\n",
    "from gensim.models.logentropy_model import LogEntropyModel\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split() if word not in string.punctuation]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text='پاکستان کے وزیراعظم عمران خان سعودی عرب کے دارالحکومت ریاض میں ملک میں سرمایہ کاری کے حوالے سے سالانہ کانفرنس میں شرکت کر رہے ہیں حکومت پاکستان کا کہنا ہے کہ سعودی عرب نے پاکستان کو معاشی بحران سے نمٹنے میں مدد کے لیے ایک سال کے لیے تین ارب ڈالر دینے پر اتفاق کیا ہے دفترِ خارجہ کی جانب سے منگل کی شب جاری ہونے والے اعلامیے میں بتایا گیا ہے کہ یہ فیصلہ'\n",
    "wordNgrams(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charNgrams(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diccionario(ruta):\n",
    "    terms = set()#Dictionary of slangs\n",
    "    try:\n",
    "        tmp = open(ruta, \"r\")     \n",
    "        while True :\n",
    "            linea = tmp.readline()                                                                                   \n",
    "            #linea = to_unicode(linea) \n",
    "            if (not linea) or (linea == \"\"):                                                                               \n",
    "                break;                                                                                                      \n",
    "            linea = linea.rstrip()\n",
    "            terms.add(linea.lower())\n",
    "        return (terms)\n",
    "    except IOError as e:\n",
    "        print (\"Error: \"+ruta+\" I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts function words n-grams with a pre-loaded dictionary\n",
    "def funcNgrams(text, n):\n",
    "    stop_words = load_diccionario('stop_words.txt')\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r\"(\\n+|\\r+|(\\r\\n)+)\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(funcNgrams(text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,cn,wn,fn):\n",
    "    text = text.lower()\n",
    "    #text=clean_text(text)\n",
    "    features = []\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in fn:\n",
    "            if n != 0:\n",
    "                features.extend(funcNgrams(text,n))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,cn,wn,fn):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    for (text) in texts:\n",
    "        features=extract_features(text,cn,wn,fn)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    #here remove text\n",
    "    cleantext=re.sub(\"\\d+\", \"0\", text)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function for reading files\n",
    "def read_txt_files(files):\n",
    "    text=[]\n",
    "    topic=[]\n",
    "    for i,file_path in enumerate(files):\n",
    "        print('news',file_path)\n",
    "        with open(file_path,'r') as infile:\n",
    "            cleantext=preprocessText(infile.read())\n",
    "            text.append(cleantext)\n",
    "            #print(file_path)\n",
    "            file_topic=''.join(re.findall('[A-Za-z]',file_path.split('/')[3].split('.')[0]))\n",
    "            #print (file_topic)\n",
    "            topic.append(file_topic)\n",
    "    return text, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news Corpus/Train/Real/bus1.txt\n",
      "news Corpus/Train/Real/bus10.txt\n",
      "news Corpus/Train/Real/bus11.txt\n",
      "news Corpus/Train/Real/bus12.txt\n",
      "news Corpus/Train/Real/bus13.txt\n",
      "news Corpus/Train/Real/bus14.txt\n",
      "news Corpus/Train/Real/bus15.txt\n",
      "news Corpus/Train/Real/bus16.txt\n",
      "news Corpus/Train/Real/bus17.txt\n",
      "news Corpus/Train/Real/bus18.txt\n",
      "news Corpus/Train/Real/bus19.txt\n",
      "news Corpus/Train/Real/bus2.txt\n",
      "news Corpus/Train/Real/bus20.txt\n",
      "news Corpus/Train/Real/bus21.txt\n",
      "news Corpus/Train/Real/bus22.txt\n",
      "news Corpus/Train/Real/bus23.txt\n",
      "news Corpus/Train/Real/bus24.txt\n",
      "news Corpus/Train/Real/bus25.txt\n",
      "news Corpus/Train/Real/bus26.txt\n",
      "news Corpus/Train/Real/bus27.txt\n",
      "news Corpus/Train/Real/bus28.txt\n",
      "news Corpus/Train/Real/bus29.txt\n",
      "news Corpus/Train/Real/bus3.txt\n",
      "news Corpus/Train/Real/bus30.txt\n",
      "news Corpus/Train/Real/bus31.txt\n",
      "news Corpus/Train/Real/bus32.txt\n",
      "news Corpus/Train/Real/bus33.txt\n",
      "news Corpus/Train/Real/bus34.txt\n",
      "news Corpus/Train/Real/bus35.txt\n",
      "news Corpus/Train/Real/bus36.txt\n",
      "news Corpus/Train/Real/bus37.txt\n",
      "news Corpus/Train/Real/bus38.txt\n",
      "news Corpus/Train/Real/bus39.txt\n",
      "news Corpus/Train/Real/bus4.txt\n",
      "news Corpus/Train/Real/bus40.txt\n",
      "news Corpus/Train/Real/bus41.txt\n",
      "news Corpus/Train/Real/bus42.txt\n",
      "news Corpus/Train/Real/bus43.txt\n",
      "news Corpus/Train/Real/bus44.txt\n",
      "news Corpus/Train/Real/bus45.txt\n",
      "news Corpus/Train/Real/bus46.txt\n",
      "news Corpus/Train/Real/bus47.txt\n",
      "news Corpus/Train/Real/bus48.txt\n",
      "news Corpus/Train/Real/bus49.txt\n",
      "news Corpus/Train/Real/bus5.txt\n",
      "news Corpus/Train/Real/bus50.txt\n",
      "news Corpus/Train/Real/bus51.txt\n",
      "news Corpus/Train/Real/bus52.txt\n",
      "news Corpus/Train/Real/bus53.txt\n",
      "news Corpus/Train/Real/bus54.txt\n",
      "news Corpus/Train/Real/bus55.txt\n",
      "news Corpus/Train/Real/bus56.txt\n",
      "news Corpus/Train/Real/bus57.txt\n",
      "news Corpus/Train/Real/bus58.txt\n",
      "news Corpus/Train/Real/bus59.txt\n",
      "news Corpus/Train/Real/bus6.txt\n",
      "news Corpus/Train/Real/bus60.txt\n",
      "news Corpus/Train/Real/bus61.txt\n",
      "news Corpus/Train/Real/bus62.txt\n",
      "news Corpus/Train/Real/bus63.txt\n",
      "news Corpus/Train/Real/bus64.txt\n",
      "news Corpus/Train/Real/bus65.txt\n",
      "news Corpus/Train/Real/bus66.txt\n",
      "news Corpus/Train/Real/bus67.txt\n",
      "news Corpus/Train/Real/bus68.txt\n",
      "news Corpus/Train/Real/bus69.txt\n",
      "news Corpus/Train/Real/bus7.txt\n",
      "news Corpus/Train/Real/bus70.txt\n",
      "news Corpus/Train/Real/bus8.txt\n",
      "news Corpus/Train/Real/bus9.txt\n",
      "news Corpus/Train/Real/hlth1.txt\n",
      "news Corpus/Train/Real/hlth10.txt\n",
      "news Corpus/Train/Real/hlth11.txt\n",
      "news Corpus/Train/Real/hlth12.txt\n",
      "news Corpus/Train/Real/hlth13.txt\n",
      "news Corpus/Train/Real/hlth14.txt\n",
      "news Corpus/Train/Real/hlth15.txt\n",
      "news Corpus/Train/Real/hlth16.txt\n",
      "news Corpus/Train/Real/hlth17.txt\n",
      "news Corpus/Train/Real/hlth18.txt\n",
      "news Corpus/Train/Real/hlth19.txt\n",
      "news Corpus/Train/Real/hlth2.txt\n",
      "news Corpus/Train/Real/hlth20.txt\n",
      "news Corpus/Train/Real/hlth21.txt\n",
      "news Corpus/Train/Real/hlth22.txt\n",
      "news Corpus/Train/Real/hlth23.txt\n",
      "news Corpus/Train/Real/hlth24.txt\n",
      "news Corpus/Train/Real/hlth25.txt\n",
      "news Corpus/Train/Real/hlth26.txt\n",
      "news Corpus/Train/Real/hlth27.txt\n",
      "news Corpus/Train/Real/hlth28.txt\n",
      "news Corpus/Train/Real/hlth29.txt\n",
      "news Corpus/Train/Real/hlth3.txt\n",
      "news Corpus/Train/Real/hlth30.txt\n",
      "news Corpus/Train/Real/hlth31.txt\n",
      "news Corpus/Train/Real/hlth32.txt\n",
      "news Corpus/Train/Real/hlth33.txt\n",
      "news Corpus/Train/Real/hlth34.txt\n",
      "news Corpus/Train/Real/hlth35.txt\n",
      "news Corpus/Train/Real/hlth36.txt\n",
      "news Corpus/Train/Real/hlth37.txt\n",
      "news Corpus/Train/Real/hlth38.txt\n",
      "news Corpus/Train/Real/hlth39.txt\n",
      "news Corpus/Train/Real/hlth4.txt\n",
      "news Corpus/Train/Real/hlth40.txt\n",
      "news Corpus/Train/Real/hlth41.txt\n",
      "news Corpus/Train/Real/hlth42.txt\n",
      "news Corpus/Train/Real/hlth43.txt\n",
      "news Corpus/Train/Real/hlth44.txt\n",
      "news Corpus/Train/Real/hlth45.txt\n",
      "news Corpus/Train/Real/hlth46.txt\n",
      "news Corpus/Train/Real/hlth47.txt\n",
      "news Corpus/Train/Real/hlth48.txt\n",
      "news Corpus/Train/Real/hlth49.txt\n",
      "news Corpus/Train/Real/hlth5.txt\n",
      "news Corpus/Train/Real/hlth50.txt\n",
      "news Corpus/Train/Real/hlth51.txt\n",
      "news Corpus/Train/Real/hlth52.txt\n",
      "news Corpus/Train/Real/hlth53.txt\n",
      "news Corpus/Train/Real/hlth54.txt\n",
      "news Corpus/Train/Real/hlth55.txt\n",
      "news Corpus/Train/Real/hlth56.txt\n",
      "news Corpus/Train/Real/hlth57.txt\n",
      "news Corpus/Train/Real/hlth58.txt\n",
      "news Corpus/Train/Real/hlth59.txt\n",
      "news Corpus/Train/Real/hlth6.txt\n",
      "news Corpus/Train/Real/hlth60.txt\n",
      "news Corpus/Train/Real/hlth61.txt\n",
      "news Corpus/Train/Real/hlth62.txt\n",
      "news Corpus/Train/Real/hlth63.txt\n",
      "news Corpus/Train/Real/hlth64.txt\n",
      "news Corpus/Train/Real/hlth65.txt\n",
      "news Corpus/Train/Real/hlth66.txt\n",
      "news Corpus/Train/Real/hlth67.txt\n",
      "news Corpus/Train/Real/hlth68.txt\n",
      "news Corpus/Train/Real/hlth69.txt\n",
      "news Corpus/Train/Real/hlth7.txt\n",
      "news Corpus/Train/Real/hlth70.txt\n",
      "news Corpus/Train/Real/hlth8.txt\n",
      "news Corpus/Train/Real/hlth9.txt\n",
      "news Corpus/Train/Real/sbz1.txt\n",
      "news Corpus/Train/Real/sbz10.txt\n",
      "news Corpus/Train/Real/sbz11.txt\n",
      "news Corpus/Train/Real/sbz12.txt\n",
      "news Corpus/Train/Real/sbz13.txt\n",
      "news Corpus/Train/Real/sbz14.txt\n",
      "news Corpus/Train/Real/sbz15.txt\n",
      "news Corpus/Train/Real/sbz16.txt\n",
      "news Corpus/Train/Real/sbz17.txt\n",
      "news Corpus/Train/Real/sbz18.txt\n",
      "news Corpus/Train/Real/sbz19.txt\n",
      "news Corpus/Train/Real/sbz2.txt\n",
      "news Corpus/Train/Real/sbz20.txt\n",
      "news Corpus/Train/Real/sbz21.txt\n",
      "news Corpus/Train/Real/sbz22.txt\n",
      "news Corpus/Train/Real/sbz23.txt\n",
      "news Corpus/Train/Real/sbz24.txt\n",
      "news Corpus/Train/Real/sbz25.txt\n",
      "news Corpus/Train/Real/sbz26.txt\n",
      "news Corpus/Train/Real/sbz27.txt\n",
      "news Corpus/Train/Real/sbz28.txt\n",
      "news Corpus/Train/Real/sbz29.txt\n",
      "news Corpus/Train/Real/sbz3.txt\n",
      "news Corpus/Train/Real/sbz30.txt\n",
      "news Corpus/Train/Real/sbz31.txt\n",
      "news Corpus/Train/Real/sbz32.txt\n",
      "news Corpus/Train/Real/sbz33.txt\n",
      "news Corpus/Train/Real/sbz34.txt\n",
      "news Corpus/Train/Real/sbz35.txt\n",
      "news Corpus/Train/Real/sbz36.txt\n",
      "news Corpus/Train/Real/sbz37.txt\n",
      "news Corpus/Train/Real/sbz38.txt\n",
      "news Corpus/Train/Real/sbz39.txt\n",
      "news Corpus/Train/Real/sbz4.txt\n",
      "news Corpus/Train/Real/sbz40.txt\n",
      "news Corpus/Train/Real/sbz41.txt\n",
      "news Corpus/Train/Real/sbz42.txt\n",
      "news Corpus/Train/Real/sbz43.txt\n",
      "news Corpus/Train/Real/sbz44.txt\n",
      "news Corpus/Train/Real/sbz45.txt\n",
      "news Corpus/Train/Real/sbz46.txt\n",
      "news Corpus/Train/Real/sbz47.txt\n",
      "news Corpus/Train/Real/sbz48.txt\n",
      "news Corpus/Train/Real/sbz49.txt\n",
      "news Corpus/Train/Real/sbz5.txt\n",
      "news Corpus/Train/Real/sbz50.txt\n",
      "news Corpus/Train/Real/sbz51.txt\n",
      "news Corpus/Train/Real/sbz52.txt\n",
      "news Corpus/Train/Real/sbz53.txt\n",
      "news Corpus/Train/Real/sbz54.txt\n",
      "news Corpus/Train/Real/sbz55.txt\n",
      "news Corpus/Train/Real/sbz56.txt\n",
      "news Corpus/Train/Real/sbz57.txt\n",
      "news Corpus/Train/Real/sbz58.txt\n",
      "news Corpus/Train/Real/sbz59.txt\n",
      "news Corpus/Train/Real/sbz6.txt\n",
      "news Corpus/Train/Real/sbz60.txt\n",
      "news Corpus/Train/Real/sbz61.txt\n",
      "news Corpus/Train/Real/sbz62.txt\n",
      "news Corpus/Train/Real/sbz63.txt\n",
      "news Corpus/Train/Real/sbz64.txt\n",
      "news Corpus/Train/Real/sbz65.txt\n",
      "news Corpus/Train/Real/sbz66.txt\n",
      "news Corpus/Train/Real/sbz67.txt\n",
      "news Corpus/Train/Real/sbz68.txt\n",
      "news Corpus/Train/Real/sbz69.txt\n",
      "news Corpus/Train/Real/sbz7.txt\n",
      "news Corpus/Train/Real/sbz70.txt\n",
      "news Corpus/Train/Real/sbz8.txt\n",
      "news Corpus/Train/Real/sbz9.txt\n",
      "news Corpus/Train/Real/sp1.txt\n",
      "news Corpus/Train/Real/sp10.txt\n",
      "news Corpus/Train/Real/sp11.txt\n",
      "news Corpus/Train/Real/sp12.txt\n",
      "news Corpus/Train/Real/sp13.txt\n",
      "news Corpus/Train/Real/sp14.txt\n",
      "news Corpus/Train/Real/sp15.txt\n",
      "news Corpus/Train/Real/sp16.txt\n",
      "news Corpus/Train/Real/sp17.txt\n",
      "news Corpus/Train/Real/sp18.txt\n",
      "news Corpus/Train/Real/sp19.txt\n",
      "news Corpus/Train/Real/sp2.txt\n",
      "news Corpus/Train/Real/sp20.txt\n",
      "news Corpus/Train/Real/sp21.txt\n",
      "news Corpus/Train/Real/sp22.txt\n",
      "news Corpus/Train/Real/sp23.txt\n",
      "news Corpus/Train/Real/sp24.txt\n",
      "news Corpus/Train/Real/sp25.txt\n",
      "news Corpus/Train/Real/sp26.txt\n",
      "news Corpus/Train/Real/sp27.txt\n",
      "news Corpus/Train/Real/sp28.txt\n",
      "news Corpus/Train/Real/sp29.txt\n",
      "news Corpus/Train/Real/sp3.txt\n",
      "news Corpus/Train/Real/sp30.txt\n",
      "news Corpus/Train/Real/sp31.txt\n",
      "news Corpus/Train/Real/sp32.txt\n",
      "news Corpus/Train/Real/sp33.txt\n",
      "news Corpus/Train/Real/sp34.txt\n",
      "news Corpus/Train/Real/sp35.txt\n",
      "news Corpus/Train/Real/sp36.txt\n",
      "news Corpus/Train/Real/sp37.txt\n",
      "news Corpus/Train/Real/sp38.txt\n",
      "news Corpus/Train/Real/sp39.txt\n",
      "news Corpus/Train/Real/sp4.txt\n",
      "news Corpus/Train/Real/sp40.txt\n",
      "news Corpus/Train/Real/sp41.txt\n",
      "news Corpus/Train/Real/sp42.txt\n",
      "news Corpus/Train/Real/sp43.txt\n",
      "news Corpus/Train/Real/sp44.txt\n",
      "news Corpus/Train/Real/sp45.txt\n",
      "news Corpus/Train/Real/sp46.txt\n",
      "news Corpus/Train/Real/sp47.txt\n",
      "news Corpus/Train/Real/sp48.txt\n",
      "news Corpus/Train/Real/sp49.txt\n",
      "news Corpus/Train/Real/sp5.txt\n",
      "news Corpus/Train/Real/sp50.txt\n",
      "news Corpus/Train/Real/sp51.txt\n",
      "news Corpus/Train/Real/sp52.txt\n",
      "news Corpus/Train/Real/sp53.txt\n",
      "news Corpus/Train/Real/sp54.txt\n",
      "news Corpus/Train/Real/sp55.txt\n",
      "news Corpus/Train/Real/sp56.txt\n",
      "news Corpus/Train/Real/sp57.txt\n",
      "news Corpus/Train/Real/sp58.txt\n",
      "news Corpus/Train/Real/sp59.txt\n",
      "news Corpus/Train/Real/sp6.txt\n",
      "news Corpus/Train/Real/sp60.txt\n",
      "news Corpus/Train/Real/sp61.txt\n",
      "news Corpus/Train/Real/sp62.txt\n",
      "news Corpus/Train/Real/sp63.txt\n",
      "news Corpus/Train/Real/sp64.txt\n",
      "news Corpus/Train/Real/sp65.txt\n",
      "news Corpus/Train/Real/sp66.txt\n",
      "news Corpus/Train/Real/sp67.txt\n",
      "news Corpus/Train/Real/sp68.txt\n",
      "news Corpus/Train/Real/sp69.txt\n",
      "news Corpus/Train/Real/sp7.txt\n",
      "news Corpus/Train/Real/sp70.txt\n",
      "news Corpus/Train/Real/sp8.txt\n",
      "news Corpus/Train/Real/sp9.txt\n",
      "news Corpus/Train/Real/tch1.txt\n",
      "news Corpus/Train/Real/tch10.txt\n",
      "news Corpus/Train/Real/tch11.txt\n",
      "news Corpus/Train/Real/tch12.txt\n",
      "news Corpus/Train/Real/tch13.txt\n",
      "news Corpus/Train/Real/tch14.txt\n",
      "news Corpus/Train/Real/tch15.txt\n",
      "news Corpus/Train/Real/tch16.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news Corpus/Train/Real/tch17.txt\n",
      "news Corpus/Train/Real/tch18.txt\n",
      "news Corpus/Train/Real/tch19.txt\n",
      "news Corpus/Train/Real/tch2.txt\n",
      "news Corpus/Train/Real/tch20.txt\n",
      "news Corpus/Train/Real/tch21.txt\n",
      "news Corpus/Train/Real/tch22.txt\n",
      "news Corpus/Train/Real/tch23.txt\n",
      "news Corpus/Train/Real/tch24.txt\n",
      "news Corpus/Train/Real/tch25.txt\n",
      "news Corpus/Train/Real/tch26.txt\n",
      "news Corpus/Train/Real/tch27.txt\n",
      "news Corpus/Train/Real/tch28.txt\n",
      "news Corpus/Train/Real/tch29.txt\n",
      "news Corpus/Train/Real/tch3.txt\n",
      "news Corpus/Train/Real/tch30.txt\n",
      "news Corpus/Train/Real/tch31.txt\n",
      "news Corpus/Train/Real/tch32.txt\n",
      "news Corpus/Train/Real/tch33.txt\n",
      "news Corpus/Train/Real/tch34.txt\n",
      "news Corpus/Train/Real/tch35.txt\n",
      "news Corpus/Train/Real/tch36.txt\n",
      "news Corpus/Train/Real/tch37.txt\n",
      "news Corpus/Train/Real/tch38.txt\n",
      "news Corpus/Train/Real/tch39.txt\n",
      "news Corpus/Train/Real/tch4.txt\n",
      "news Corpus/Train/Real/tch40.txt\n",
      "news Corpus/Train/Real/tch41.txt\n",
      "news Corpus/Train/Real/tch42.txt\n",
      "news Corpus/Train/Real/tch43.txt\n",
      "news Corpus/Train/Real/tch44.txt\n",
      "news Corpus/Train/Real/tch45.txt\n",
      "news Corpus/Train/Real/tch46.txt\n",
      "news Corpus/Train/Real/tch47.txt\n",
      "news Corpus/Train/Real/tch48.txt\n",
      "news Corpus/Train/Real/tch49.txt\n",
      "news Corpus/Train/Real/tch5.txt\n",
      "news Corpus/Train/Real/tch50.txt\n",
      "news Corpus/Train/Real/tch51.txt\n",
      "news Corpus/Train/Real/tch52.txt\n",
      "news Corpus/Train/Real/tch53.txt\n",
      "news Corpus/Train/Real/tch54.txt\n",
      "news Corpus/Train/Real/tch55.txt\n",
      "news Corpus/Train/Real/tch56.txt\n",
      "news Corpus/Train/Real/tch57.txt\n",
      "news Corpus/Train/Real/tch58.txt\n",
      "news Corpus/Train/Real/tch59.txt\n",
      "news Corpus/Train/Real/tch6.txt\n",
      "news Corpus/Train/Real/tch60.txt\n",
      "news Corpus/Train/Real/tch61.txt\n",
      "news Corpus/Train/Real/tch62.txt\n",
      "news Corpus/Train/Real/tch63.txt\n",
      "news Corpus/Train/Real/tch64.txt\n",
      "news Corpus/Train/Real/tch65.txt\n",
      "news Corpus/Train/Real/tch66.txt\n",
      "news Corpus/Train/Real/tch67.txt\n",
      "news Corpus/Train/Real/tch68.txt\n",
      "news Corpus/Train/Real/tch69.txt\n",
      "news Corpus/Train/Real/tch7.txt\n",
      "news Corpus/Train/Real/tch70.txt\n",
      "news Corpus/Train/Real/tch8.txt\n",
      "news Corpus/Train/Real/tch9.txt\n",
      "news Corpus/Train/Fake/bus1.txt\n",
      "news Corpus/Train/Fake/bus10.txt\n",
      "news Corpus/Train/Fake/bus11.txt\n",
      "news Corpus/Train/Fake/bus14.txt\n",
      "news Corpus/Train/Fake/bus15.txt\n",
      "news Corpus/Train/Fake/bus17.txt\n",
      "news Corpus/Train/Fake/bus2.txt\n",
      "news Corpus/Train/Fake/bus20.txt\n",
      "news Corpus/Train/Fake/bus22.txt\n",
      "news Corpus/Train/Fake/bus26.txt\n",
      "news Corpus/Train/Fake/bus29.txt\n",
      "news Corpus/Train/Fake/bus30.txt\n",
      "news Corpus/Train/Fake/bus33.txt\n",
      "news Corpus/Train/Fake/bus34.txt\n",
      "news Corpus/Train/Fake/bus36.txt\n",
      "news Corpus/Train/Fake/bus37.txt\n",
      "news Corpus/Train/Fake/bus38.txt\n",
      "news Corpus/Train/Fake/bus4.txt\n",
      "news Corpus/Train/Fake/bus40.txt\n",
      "news Corpus/Train/Fake/bus42.txt\n",
      "news Corpus/Train/Fake/bus44.txt\n",
      "news Corpus/Train/Fake/bus46.txt\n",
      "news Corpus/Train/Fake/bus47.txt\n",
      "news Corpus/Train/Fake/bus49.txt\n",
      "news Corpus/Train/Fake/bus5.txt\n",
      "news Corpus/Train/Fake/bus51.txt\n",
      "news Corpus/Train/Fake/bus53.txt\n",
      "news Corpus/Train/Fake/bus54.txt\n",
      "news Corpus/Train/Fake/bus55.txt\n",
      "news Corpus/Train/Fake/bus6.txt\n",
      "news Corpus/Train/Fake/bus60.txt\n",
      "news Corpus/Train/Fake/bus62.txt\n",
      "news Corpus/Train/Fake/bus66.txt\n",
      "news Corpus/Train/Fake/bus67.txt\n",
      "news Corpus/Train/Fake/bus7.txt\n",
      "news Corpus/Train/Fake/bus70.txt\n",
      "news Corpus/Train/Fake/hlth1.txt\n",
      "news Corpus/Train/Fake/hlth10.txt\n",
      "news Corpus/Train/Fake/hlth11.txt\n",
      "news Corpus/Train/Fake/hlth12.txt\n",
      "news Corpus/Train/Fake/hlth13.txt\n",
      "news Corpus/Train/Fake/hlth14.txt\n",
      "news Corpus/Train/Fake/hlth15.txt\n",
      "news Corpus/Train/Fake/hlth16.txt\n",
      "news Corpus/Train/Fake/hlth17.txt\n",
      "news Corpus/Train/Fake/hlth18.txt\n",
      "news Corpus/Train/Fake/hlth19.txt\n",
      "news Corpus/Train/Fake/hlth2.txt\n",
      "news Corpus/Train/Fake/hlth20.txt\n",
      "news Corpus/Train/Fake/hlth21.txt\n",
      "news Corpus/Train/Fake/hlth22.txt\n",
      "news Corpus/Train/Fake/hlth23.txt\n",
      "news Corpus/Train/Fake/hlth24.txt\n",
      "news Corpus/Train/Fake/hlth25.txt\n",
      "news Corpus/Train/Fake/hlth26.txt\n",
      "news Corpus/Train/Fake/hlth27.txt\n",
      "news Corpus/Train/Fake/hlth28.txt\n",
      "news Corpus/Train/Fake/hlth29.txt\n",
      "news Corpus/Train/Fake/hlth3.txt\n",
      "news Corpus/Train/Fake/hlth30.txt\n",
      "news Corpus/Train/Fake/hlth31.txt\n",
      "news Corpus/Train/Fake/hlth32.txt\n",
      "news Corpus/Train/Fake/hlth33.txt\n",
      "news Corpus/Train/Fake/hlth34.txt\n",
      "news Corpus/Train/Fake/hlth35.txt\n",
      "news Corpus/Train/Fake/hlth36.txt\n",
      "news Corpus/Train/Fake/hlth37.txt\n",
      "news Corpus/Train/Fake/hlth38.txt\n",
      "news Corpus/Train/Fake/hlth39.txt\n",
      "news Corpus/Train/Fake/hlth4.txt\n",
      "news Corpus/Train/Fake/hlth40.txt\n",
      "news Corpus/Train/Fake/hlth41.txt\n",
      "news Corpus/Train/Fake/hlth42.txt\n",
      "news Corpus/Train/Fake/hlth43.txt\n",
      "news Corpus/Train/Fake/hlth44.txt\n",
      "news Corpus/Train/Fake/hlth45.txt\n",
      "news Corpus/Train/Fake/hlth46.txt\n",
      "news Corpus/Train/Fake/hlth47.txt\n",
      "news Corpus/Train/Fake/hlth48.txt\n",
      "news Corpus/Train/Fake/hlth49.txt\n",
      "news Corpus/Train/Fake/hlth5.txt\n",
      "news Corpus/Train/Fake/hlth50.txt\n",
      "news Corpus/Train/Fake/hlth51.txt\n",
      "news Corpus/Train/Fake/hlth52.txt\n",
      "news Corpus/Train/Fake/hlth53.txt\n",
      "news Corpus/Train/Fake/hlth54.txt\n",
      "news Corpus/Train/Fake/hlth55.txt\n",
      "news Corpus/Train/Fake/hlth56.txt\n",
      "news Corpus/Train/Fake/hlth57.txt\n",
      "news Corpus/Train/Fake/hlth58.txt\n",
      "news Corpus/Train/Fake/hlth59.txt\n",
      "news Corpus/Train/Fake/hlth6.txt\n",
      "news Corpus/Train/Fake/hlth60.txt\n",
      "news Corpus/Train/Fake/hlth61.txt\n",
      "news Corpus/Train/Fake/hlth62.txt\n",
      "news Corpus/Train/Fake/hlth63.txt\n",
      "news Corpus/Train/Fake/hlth64.txt\n",
      "news Corpus/Train/Fake/hlth65.txt\n",
      "news Corpus/Train/Fake/hlth66.txt\n",
      "news Corpus/Train/Fake/hlth67.txt\n",
      "news Corpus/Train/Fake/hlth68.txt\n",
      "news Corpus/Train/Fake/hlth69.txt\n",
      "news Corpus/Train/Fake/hlth7.txt\n",
      "news Corpus/Train/Fake/hlth70.txt\n",
      "news Corpus/Train/Fake/hlth8.txt\n",
      "news Corpus/Train/Fake/hlth9.txt\n",
      "news Corpus/Train/Fake/sbz1.txt\n",
      "news Corpus/Train/Fake/sbz10.txt\n",
      "news Corpus/Train/Fake/sbz11.txt\n",
      "news Corpus/Train/Fake/sbz12.txt\n",
      "news Corpus/Train/Fake/sbz13.txt\n",
      "news Corpus/Train/Fake/sbz14.txt\n",
      "news Corpus/Train/Fake/sbz15.txt\n",
      "news Corpus/Train/Fake/sbz16.txt\n",
      "news Corpus/Train/Fake/sbz17.txt\n",
      "news Corpus/Train/Fake/sbz18.txt\n",
      "news Corpus/Train/Fake/sbz19.txt\n",
      "news Corpus/Train/Fake/sbz2.txt\n",
      "news Corpus/Train/Fake/sbz20.txt\n",
      "news Corpus/Train/Fake/sbz21.txt\n",
      "news Corpus/Train/Fake/sbz22.txt\n",
      "news Corpus/Train/Fake/sbz23.txt\n",
      "news Corpus/Train/Fake/sbz24.txt\n",
      "news Corpus/Train/Fake/sbz25.txt\n",
      "news Corpus/Train/Fake/sbz26.txt\n",
      "news Corpus/Train/Fake/sbz27.txt\n",
      "news Corpus/Train/Fake/sbz28.txt\n",
      "news Corpus/Train/Fake/sbz29.txt\n",
      "news Corpus/Train/Fake/sbz3.txt\n",
      "news Corpus/Train/Fake/sbz30.txt\n",
      "news Corpus/Train/Fake/sbz31.txt\n",
      "news Corpus/Train/Fake/sbz32.txt\n",
      "news Corpus/Train/Fake/sbz33.txt\n",
      "news Corpus/Train/Fake/sbz34.txt\n",
      "news Corpus/Train/Fake/sbz35.txt\n",
      "news Corpus/Train/Fake/sbz36.txt\n",
      "news Corpus/Train/Fake/sbz37.txt\n",
      "news Corpus/Train/Fake/sbz38.txt\n",
      "news Corpus/Train/Fake/sbz39.txt\n",
      "news Corpus/Train/Fake/sbz4.txt\n",
      "news Corpus/Train/Fake/sbz40.txt\n",
      "news Corpus/Train/Fake/sbz41.txt\n",
      "news Corpus/Train/Fake/sbz42.txt\n",
      "news Corpus/Train/Fake/sbz43.txt\n",
      "news Corpus/Train/Fake/sbz44.txt\n",
      "news Corpus/Train/Fake/sbz45.txt\n",
      "news Corpus/Train/Fake/sbz46.txt\n",
      "news Corpus/Train/Fake/sbz47.txt\n",
      "news Corpus/Train/Fake/sbz48.txt\n",
      "news Corpus/Train/Fake/sbz49.txt\n",
      "news Corpus/Train/Fake/sbz5.txt\n",
      "news Corpus/Train/Fake/sbz50.txt\n",
      "news Corpus/Train/Fake/sbz51.txt\n",
      "news Corpus/Train/Fake/sbz52.txt\n",
      "news Corpus/Train/Fake/sbz53.txt\n",
      "news Corpus/Train/Fake/sbz54.txt\n",
      "news Corpus/Train/Fake/sbz55.txt\n",
      "news Corpus/Train/Fake/sbz56.txt\n",
      "news Corpus/Train/Fake/sbz57.txt\n",
      "news Corpus/Train/Fake/sbz58.txt\n",
      "news Corpus/Train/Fake/sbz59.txt\n",
      "news Corpus/Train/Fake/sbz6.txt\n",
      "news Corpus/Train/Fake/sbz60.txt\n",
      "news Corpus/Train/Fake/sbz61.txt\n",
      "news Corpus/Train/Fake/sbz62.txt\n",
      "news Corpus/Train/Fake/sbz63.txt\n",
      "news Corpus/Train/Fake/sbz64.txt\n",
      "news Corpus/Train/Fake/sbz65.txt\n",
      "news Corpus/Train/Fake/sbz66.txt\n",
      "news Corpus/Train/Fake/sbz67.txt\n",
      "news Corpus/Train/Fake/sbz68.txt\n",
      "news Corpus/Train/Fake/sbz69.txt\n",
      "news Corpus/Train/Fake/sbz7.txt\n",
      "news Corpus/Train/Fake/sbz70.txt\n",
      "news Corpus/Train/Fake/sbz8.txt\n",
      "news Corpus/Train/Fake/sbz9.txt\n",
      "news Corpus/Train/Fake/sp10.txt\n",
      "news Corpus/Train/Fake/sp11.txt\n",
      "news Corpus/Train/Fake/sp12.txt\n",
      "news Corpus/Train/Fake/sp14.txt\n",
      "news Corpus/Train/Fake/sp17.txt\n",
      "news Corpus/Train/Fake/sp18.txt\n",
      "news Corpus/Train/Fake/sp2.txt\n",
      "news Corpus/Train/Fake/sp22.txt\n",
      "news Corpus/Train/Fake/sp24.txt\n",
      "news Corpus/Train/Fake/sp26.txt\n",
      "news Corpus/Train/Fake/sp27.txt\n",
      "news Corpus/Train/Fake/sp28.txt\n",
      "news Corpus/Train/Fake/sp3.txt\n",
      "news Corpus/Train/Fake/sp30.txt\n",
      "news Corpus/Train/Fake/sp31.txt\n",
      "news Corpus/Train/Fake/sp32.txt\n",
      "news Corpus/Train/Fake/sp34.txt\n",
      "news Corpus/Train/Fake/sp35.txt\n",
      "news Corpus/Train/Fake/sp36.txt\n",
      "news Corpus/Train/Fake/sp37.txt\n",
      "news Corpus/Train/Fake/sp40.txt\n",
      "news Corpus/Train/Fake/sp41.txt\n",
      "news Corpus/Train/Fake/sp42.txt\n",
      "news Corpus/Train/Fake/sp43.txt\n",
      "news Corpus/Train/Fake/sp45.txt\n",
      "news Corpus/Train/Fake/sp46.txt\n",
      "news Corpus/Train/Fake/sp47.txt\n",
      "news Corpus/Train/Fake/sp48.txt\n",
      "news Corpus/Train/Fake/sp49.txt\n",
      "news Corpus/Train/Fake/sp5.txt\n",
      "news Corpus/Train/Fake/sp50.txt\n",
      "news Corpus/Train/Fake/sp51.txt\n",
      "news Corpus/Train/Fake/sp52.txt\n",
      "news Corpus/Train/Fake/sp53.txt\n",
      "news Corpus/Train/Fake/sp58.txt\n",
      "news Corpus/Train/Fake/sp60.txt\n",
      "news Corpus/Train/Fake/sp62.txt\n",
      "news Corpus/Train/Fake/sp63.txt\n",
      "news Corpus/Train/Fake/sp65.txt\n",
      "news Corpus/Train/Fake/sp68.txt\n",
      "news Corpus/Train/Fake/sp8.txt\n",
      "news Corpus/Train/Fake/sp9.txt\n",
      "news Corpus/Train/Fake/tch1.txt\n",
      "news Corpus/Train/Fake/tch10.txt\n",
      "news Corpus/Train/Fake/tch11.txt\n",
      "news Corpus/Train/Fake/tch12.txt\n",
      "news Corpus/Train/Fake/tch13.txt\n",
      "news Corpus/Train/Fake/tch14.txt\n",
      "news Corpus/Train/Fake/tch15.txt\n",
      "news Corpus/Train/Fake/tch16.txt\n",
      "news Corpus/Train/Fake/tch17.txt\n",
      "news Corpus/Train/Fake/tch18.txt\n",
      "news Corpus/Train/Fake/tch19.txt\n",
      "news Corpus/Train/Fake/tch2.txt\n",
      "news Corpus/Train/Fake/tch20.txt\n",
      "news Corpus/Train/Fake/tch21.txt\n",
      "news Corpus/Train/Fake/tch22.txt\n",
      "news Corpus/Train/Fake/tch23.txt\n",
      "news Corpus/Train/Fake/tch24.txt\n",
      "news Corpus/Train/Fake/tch25.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news Corpus/Train/Fake/tch26.txt\n",
      "news Corpus/Train/Fake/tch27.txt\n",
      "news Corpus/Train/Fake/tch28.txt\n",
      "news Corpus/Train/Fake/tch29.txt\n",
      "news Corpus/Train/Fake/tch3.txt\n",
      "news Corpus/Train/Fake/tch30.txt\n",
      "news Corpus/Train/Fake/tch31.txt\n",
      "news Corpus/Train/Fake/tch32.txt\n",
      "news Corpus/Train/Fake/tch33.txt\n",
      "news Corpus/Train/Fake/tch34.txt\n",
      "news Corpus/Train/Fake/tch35.txt\n",
      "news Corpus/Train/Fake/tch36.txt\n",
      "news Corpus/Train/Fake/tch37.txt\n",
      "news Corpus/Train/Fake/tch38.txt\n",
      "news Corpus/Train/Fake/tch39.txt\n",
      "news Corpus/Train/Fake/tch4.txt\n",
      "news Corpus/Train/Fake/tch40.txt\n",
      "news Corpus/Train/Fake/tch41.txt\n",
      "news Corpus/Train/Fake/tch42.txt\n",
      "news Corpus/Train/Fake/tch43.txt\n",
      "news Corpus/Train/Fake/tch44.txt\n",
      "news Corpus/Train/Fake/tch45.txt\n",
      "news Corpus/Train/Fake/tch46.txt\n",
      "news Corpus/Train/Fake/tch47.txt\n",
      "news Corpus/Train/Fake/tch48.txt\n",
      "news Corpus/Train/Fake/tch49.txt\n",
      "news Corpus/Train/Fake/tch5.txt\n",
      "news Corpus/Train/Fake/tch50.txt\n",
      "news Corpus/Train/Fake/tch51.txt\n",
      "news Corpus/Train/Fake/tch52.txt\n",
      "news Corpus/Train/Fake/tch53.txt\n",
      "news Corpus/Train/Fake/tch54.txt\n",
      "news Corpus/Train/Fake/tch55.txt\n",
      "news Corpus/Train/Fake/tch56.txt\n",
      "news Corpus/Train/Fake/tch57.txt\n",
      "news Corpus/Train/Fake/tch58.txt\n",
      "news Corpus/Train/Fake/tch59.txt\n",
      "news Corpus/Train/Fake/tch6.txt\n",
      "news Corpus/Train/Fake/tch60.txt\n",
      "news Corpus/Train/Fake/tch61.txt\n",
      "news Corpus/Train/Fake/tch62.txt\n",
      "news Corpus/Train/Fake/tch63.txt\n",
      "news Corpus/Train/Fake/tch64.txt\n",
      "news Corpus/Train/Fake/tch65.txt\n",
      "news Corpus/Train/Fake/tch66.txt\n",
      "news Corpus/Train/Fake/tch67.txt\n",
      "news Corpus/Train/Fake/tch68.txt\n",
      "news Corpus/Train/Fake/tch69.txt\n",
      "news Corpus/Train/Fake/tch7.txt\n",
      "news Corpus/Train/Fake/tch70.txt\n",
      "news Corpus/Train/Fake/tch8.txt\n",
      "news Corpus/Train/Fake/tch9.txt\n"
     ]
    }
   ],
   "source": [
    "#reading the path of real and fake news for training\n",
    "train_path_real='Corpus/Train/Real/'\n",
    "train_path_fake='Corpus/Train/Fake/'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(sorted(glob.glob(train_path_real+'*.txt')))\n",
    "fake_news, fake_news_topics = read_txt_files(sorted(glob.glob(train_path_fake+'*.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contatenating real and fake news in one variable for training\n",
    "train_texts = np.concatenate((real_news, fake_news))\n",
    "\n",
    "train_labels_real = np.concatenate((np.ones(len(real_news)), np.zeros(len(fake_news))))\n",
    "# swapped real and fake \n",
    "train_labels_fake = np.concatenate((np.zeros(len(real_news)), np.ones(len(fake_news))))\n",
    "\n",
    "train_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Train:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news Corpus/Test/Real/bus100.txt\n",
      "news Corpus/Test/Real/bus71.txt\n",
      "news Corpus/Test/Real/bus72.txt\n",
      "news Corpus/Test/Real/bus73.txt\n",
      "news Corpus/Test/Real/bus74.txt\n",
      "news Corpus/Test/Real/bus75.txt\n",
      "news Corpus/Test/Real/bus76.txt\n",
      "news Corpus/Test/Real/bus77.txt\n",
      "news Corpus/Test/Real/bus78.txt\n",
      "news Corpus/Test/Real/bus79.txt\n",
      "news Corpus/Test/Real/bus80.txt\n",
      "news Corpus/Test/Real/bus81.txt\n",
      "news Corpus/Test/Real/bus82.txt\n",
      "news Corpus/Test/Real/bus83.txt\n",
      "news Corpus/Test/Real/bus84.txt\n",
      "news Corpus/Test/Real/bus85.txt\n",
      "news Corpus/Test/Real/bus86.txt\n",
      "news Corpus/Test/Real/bus87.txt\n",
      "news Corpus/Test/Real/bus88.txt\n",
      "news Corpus/Test/Real/bus89.txt\n",
      "news Corpus/Test/Real/bus90.txt\n",
      "news Corpus/Test/Real/bus91.txt\n",
      "news Corpus/Test/Real/bus92.txt\n",
      "news Corpus/Test/Real/bus93.txt\n",
      "news Corpus/Test/Real/bus94.txt\n",
      "news Corpus/Test/Real/bus95.txt\n",
      "news Corpus/Test/Real/bus96.txt\n",
      "news Corpus/Test/Real/bus97.txt\n",
      "news Corpus/Test/Real/bus98.txt\n",
      "news Corpus/Test/Real/bus99.txt\n",
      "news Corpus/Test/Real/hlth100.txt\n",
      "news Corpus/Test/Real/hlth71.txt\n",
      "news Corpus/Test/Real/hlth72.txt\n",
      "news Corpus/Test/Real/hlth73.txt\n",
      "news Corpus/Test/Real/hlth74.txt\n",
      "news Corpus/Test/Real/hlth75.txt\n",
      "news Corpus/Test/Real/hlth76.txt\n",
      "news Corpus/Test/Real/hlth77.txt\n",
      "news Corpus/Test/Real/hlth78.txt\n",
      "news Corpus/Test/Real/hlth79.txt\n",
      "news Corpus/Test/Real/hlth80.txt\n",
      "news Corpus/Test/Real/hlth81.txt\n",
      "news Corpus/Test/Real/hlth82.txt\n",
      "news Corpus/Test/Real/hlth83.txt\n",
      "news Corpus/Test/Real/hlth84.txt\n",
      "news Corpus/Test/Real/hlth85.txt\n",
      "news Corpus/Test/Real/hlth86.txt\n",
      "news Corpus/Test/Real/hlth87.txt\n",
      "news Corpus/Test/Real/hlth88.txt\n",
      "news Corpus/Test/Real/hlth89.txt\n",
      "news Corpus/Test/Real/hlth90.txt\n",
      "news Corpus/Test/Real/hlth91.txt\n",
      "news Corpus/Test/Real/hlth92.txt\n",
      "news Corpus/Test/Real/hlth93.txt\n",
      "news Corpus/Test/Real/hlth94.txt\n",
      "news Corpus/Test/Real/hlth95.txt\n",
      "news Corpus/Test/Real/hlth96.txt\n",
      "news Corpus/Test/Real/hlth97.txt\n",
      "news Corpus/Test/Real/hlth98.txt\n",
      "news Corpus/Test/Real/hlth99.txt\n",
      "news Corpus/Test/Real/sbz100.txt\n",
      "news Corpus/Test/Real/sbz71.txt\n",
      "news Corpus/Test/Real/sbz72.txt\n",
      "news Corpus/Test/Real/sbz73.txt\n",
      "news Corpus/Test/Real/sbz74.txt\n",
      "news Corpus/Test/Real/sbz75.txt\n",
      "news Corpus/Test/Real/sbz76.txt\n",
      "news Corpus/Test/Real/sbz77.txt\n",
      "news Corpus/Test/Real/sbz78.txt\n",
      "news Corpus/Test/Real/sbz79.txt\n",
      "news Corpus/Test/Real/sbz80.txt\n",
      "news Corpus/Test/Real/sbz81.txt\n",
      "news Corpus/Test/Real/sbz82.txt\n",
      "news Corpus/Test/Real/sbz83.txt\n",
      "news Corpus/Test/Real/sbz84.txt\n",
      "news Corpus/Test/Real/sbz85.txt\n",
      "news Corpus/Test/Real/sbz86.txt\n",
      "news Corpus/Test/Real/sbz87.txt\n",
      "news Corpus/Test/Real/sbz88.txt\n",
      "news Corpus/Test/Real/sbz89.txt\n",
      "news Corpus/Test/Real/sbz90.txt\n",
      "news Corpus/Test/Real/sbz91.txt\n",
      "news Corpus/Test/Real/sbz92.txt\n",
      "news Corpus/Test/Real/sbz93.txt\n",
      "news Corpus/Test/Real/sbz94.txt\n",
      "news Corpus/Test/Real/sbz95.txt\n",
      "news Corpus/Test/Real/sbz96.txt\n",
      "news Corpus/Test/Real/sbz97.txt\n",
      "news Corpus/Test/Real/sbz98.txt\n",
      "news Corpus/Test/Real/sbz99.txt\n",
      "news Corpus/Test/Real/sp100.txt\n",
      "news Corpus/Test/Real/sp71.txt\n",
      "news Corpus/Test/Real/sp72.txt\n",
      "news Corpus/Test/Real/sp73.txt\n",
      "news Corpus/Test/Real/sp74.txt\n",
      "news Corpus/Test/Real/sp75.txt\n",
      "news Corpus/Test/Real/sp76.txt\n",
      "news Corpus/Test/Real/sp77.txt\n",
      "news Corpus/Test/Real/sp78.txt\n",
      "news Corpus/Test/Real/sp79.txt\n",
      "news Corpus/Test/Real/sp80.txt\n",
      "news Corpus/Test/Real/sp81.txt\n",
      "news Corpus/Test/Real/sp82.txt\n",
      "news Corpus/Test/Real/sp83.txt\n",
      "news Corpus/Test/Real/sp84.txt\n",
      "news Corpus/Test/Real/sp85.txt\n",
      "news Corpus/Test/Real/sp86.txt\n",
      "news Corpus/Test/Real/sp87.txt\n",
      "news Corpus/Test/Real/sp88.txt\n",
      "news Corpus/Test/Real/sp89.txt\n",
      "news Corpus/Test/Real/sp90.txt\n",
      "news Corpus/Test/Real/sp91.txt\n",
      "news Corpus/Test/Real/sp92.txt\n",
      "news Corpus/Test/Real/sp93.txt\n",
      "news Corpus/Test/Real/sp94.txt\n",
      "news Corpus/Test/Real/sp95.txt\n",
      "news Corpus/Test/Real/sp96.txt\n",
      "news Corpus/Test/Real/sp97.txt\n",
      "news Corpus/Test/Real/sp98.txt\n",
      "news Corpus/Test/Real/sp99.txt\n",
      "news Corpus/Test/Real/tch100.txt\n",
      "news Corpus/Test/Real/tch71.txt\n",
      "news Corpus/Test/Real/tch72.txt\n",
      "news Corpus/Test/Real/tch73.txt\n",
      "news Corpus/Test/Real/tch74.txt\n",
      "news Corpus/Test/Real/tch75.txt\n",
      "news Corpus/Test/Real/tch76.txt\n",
      "news Corpus/Test/Real/tch77.txt\n",
      "news Corpus/Test/Real/tch78.txt\n",
      "news Corpus/Test/Real/tch79.txt\n",
      "news Corpus/Test/Real/tch80.txt\n",
      "news Corpus/Test/Real/tch81.txt\n",
      "news Corpus/Test/Real/tch82.txt\n",
      "news Corpus/Test/Real/tch83.txt\n",
      "news Corpus/Test/Real/tch84.txt\n",
      "news Corpus/Test/Real/tch85.txt\n",
      "news Corpus/Test/Real/tch86.txt\n",
      "news Corpus/Test/Real/tch87.txt\n",
      "news Corpus/Test/Real/tch88.txt\n",
      "news Corpus/Test/Real/tch89.txt\n",
      "news Corpus/Test/Real/tch90.txt\n",
      "news Corpus/Test/Real/tch91.txt\n",
      "news Corpus/Test/Real/tch92.txt\n",
      "news Corpus/Test/Real/tch93.txt\n",
      "news Corpus/Test/Real/tch94.txt\n",
      "news Corpus/Test/Real/tch95.txt\n",
      "news Corpus/Test/Real/tch96.txt\n",
      "news Corpus/Test/Real/tch97.txt\n",
      "news Corpus/Test/Real/tch98.txt\n",
      "news Corpus/Test/Real/tch99.txt\n",
      "news Corpus/Test/Fake/bus73.txt\n",
      "news Corpus/Test/Fake/bus74.txt\n",
      "news Corpus/Test/Fake/bus75.txt\n",
      "news Corpus/Test/Fake/bus76.txt\n",
      "news Corpus/Test/Fake/bus79.txt\n",
      "news Corpus/Test/Fake/bus80.txt\n",
      "news Corpus/Test/Fake/bus81.txt\n",
      "news Corpus/Test/Fake/bus85.txt\n",
      "news Corpus/Test/Fake/bus86.txt\n",
      "news Corpus/Test/Fake/bus91.txt\n",
      "news Corpus/Test/Fake/bus92.txt\n",
      "news Corpus/Test/Fake/bus93.txt\n",
      "news Corpus/Test/Fake/bus94.txt\n",
      "news Corpus/Test/Fake/bus98.txt\n",
      "news Corpus/Test/Fake/hlth100.txt\n",
      "news Corpus/Test/Fake/hlth71.txt\n",
      "news Corpus/Test/Fake/hlth72.txt\n",
      "news Corpus/Test/Fake/hlth73.txt\n",
      "news Corpus/Test/Fake/hlth74.txt\n",
      "news Corpus/Test/Fake/hlth75.txt\n",
      "news Corpus/Test/Fake/hlth76.txt\n",
      "news Corpus/Test/Fake/hlth77.txt\n",
      "news Corpus/Test/Fake/hlth78.txt\n",
      "news Corpus/Test/Fake/hlth79.txt\n",
      "news Corpus/Test/Fake/hlth80.txt\n",
      "news Corpus/Test/Fake/hlth81.txt\n",
      "news Corpus/Test/Fake/hlth82.txt\n",
      "news Corpus/Test/Fake/hlth83.txt\n",
      "news Corpus/Test/Fake/hlth84.txt\n",
      "news Corpus/Test/Fake/hlth85.txt\n",
      "news Corpus/Test/Fake/hlth86.txt\n",
      "news Corpus/Test/Fake/hlth87.txt\n",
      "news Corpus/Test/Fake/hlth88.txt\n",
      "news Corpus/Test/Fake/hlth89.txt\n",
      "news Corpus/Test/Fake/hlth90.txt\n",
      "news Corpus/Test/Fake/hlth91.txt\n",
      "news Corpus/Test/Fake/hlth92.txt\n",
      "news Corpus/Test/Fake/hlth93.txt\n",
      "news Corpus/Test/Fake/hlth94.txt\n",
      "news Corpus/Test/Fake/hlth95.txt\n",
      "news Corpus/Test/Fake/hlth96.txt\n",
      "news Corpus/Test/Fake/hlth97.txt\n",
      "news Corpus/Test/Fake/hlth98.txt\n",
      "news Corpus/Test/Fake/hlth99.txt\n",
      "news Corpus/Test/Fake/sbz100.txt\n",
      "news Corpus/Test/Fake/sbz71.txt\n",
      "news Corpus/Test/Fake/sbz72.txt\n",
      "news Corpus/Test/Fake/sbz73.txt\n",
      "news Corpus/Test/Fake/sbz74.txt\n",
      "news Corpus/Test/Fake/sbz75.txt\n",
      "news Corpus/Test/Fake/sbz76.txt\n",
      "news Corpus/Test/Fake/sbz77.txt\n",
      "news Corpus/Test/Fake/sbz78.txt\n",
      "news Corpus/Test/Fake/sbz79.txt\n",
      "news Corpus/Test/Fake/sbz80.txt\n",
      "news Corpus/Test/Fake/sbz81.txt\n",
      "news Corpus/Test/Fake/sbz82.txt\n",
      "news Corpus/Test/Fake/sbz83.txt\n",
      "news Corpus/Test/Fake/sbz84.txt\n",
      "news Corpus/Test/Fake/sbz85.txt\n",
      "news Corpus/Test/Fake/sbz86.txt\n",
      "news Corpus/Test/Fake/sbz87.txt\n",
      "news Corpus/Test/Fake/sbz88.txt\n",
      "news Corpus/Test/Fake/sbz89.txt\n",
      "news Corpus/Test/Fake/sbz90.txt\n",
      "news Corpus/Test/Fake/sbz91.txt\n",
      "news Corpus/Test/Fake/sbz92.txt\n",
      "news Corpus/Test/Fake/sbz93.txt\n",
      "news Corpus/Test/Fake/sbz94.txt\n",
      "news Corpus/Test/Fake/sbz95.txt\n",
      "news Corpus/Test/Fake/sbz96.txt\n",
      "news Corpus/Test/Fake/sbz97.txt\n",
      "news Corpus/Test/Fake/sbz98.txt\n",
      "news Corpus/Test/Fake/sbz99.txt\n",
      "news Corpus/Test/Fake/sp74.txt\n",
      "news Corpus/Test/Fake/sp76.txt\n",
      "news Corpus/Test/Fake/sp77.txt\n",
      "news Corpus/Test/Fake/sp85.txt\n",
      "news Corpus/Test/Fake/sp88.txt\n",
      "news Corpus/Test/Fake/sp89.txt\n",
      "news Corpus/Test/Fake/sp92.txt\n",
      "news Corpus/Test/Fake/sp93.txt\n",
      "news Corpus/Test/Fake/tch100.txt\n",
      "news Corpus/Test/Fake/tch71.txt\n",
      "news Corpus/Test/Fake/tch72.txt\n",
      "news Corpus/Test/Fake/tch73.txt\n",
      "news Corpus/Test/Fake/tch74.txt\n",
      "news Corpus/Test/Fake/tch75.txt\n",
      "news Corpus/Test/Fake/tch76.txt\n",
      "news Corpus/Test/Fake/tch77.txt\n",
      "news Corpus/Test/Fake/tch78.txt\n",
      "news Corpus/Test/Fake/tch79.txt\n",
      "news Corpus/Test/Fake/tch80.txt\n",
      "news Corpus/Test/Fake/tch81.txt\n",
      "news Corpus/Test/Fake/tch82.txt\n",
      "news Corpus/Test/Fake/tch83.txt\n",
      "news Corpus/Test/Fake/tch84.txt\n",
      "news Corpus/Test/Fake/tch85.txt\n",
      "news Corpus/Test/Fake/tch86.txt\n",
      "news Corpus/Test/Fake/tch87.txt\n",
      "news Corpus/Test/Fake/tch88.txt\n",
      "news Corpus/Test/Fake/tch89.txt\n",
      "news Corpus/Test/Fake/tch90.txt\n",
      "news Corpus/Test/Fake/tch91.txt\n",
      "news Corpus/Test/Fake/tch92.txt\n",
      "news Corpus/Test/Fake/tch93.txt\n",
      "news Corpus/Test/Fake/tch94.txt\n",
      "news Corpus/Test/Fake/tch95.txt\n",
      "news Corpus/Test/Fake/tch96.txt\n",
      "news Corpus/Test/Fake/tch97.txt\n",
      "news Corpus/Test/Fake/tch98.txt\n",
      "news Corpus/Test/Fake/tch99.txt\n"
     ]
    }
   ],
   "source": [
    "#reading the path of real and fake news for testing\n",
    "test_path_real='Corpus/Test/Real/'\n",
    "test_path_fake='Corpus/Test/Fake/'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(sorted(glob.glob(test_path_real+'*.txt')))\n",
    "fake_news, fake_news_topics = read_txt_files(sorted(glob.glob(test_path_fake+'*.txt')))\n",
    "\n",
    "#contatenating real and fake news in one variable for testing\n",
    "test_texts = np.concatenate((real_news, fake_news))\n",
    "test_labels_real = np.concatenate((np.ones(len(real_news)), np.zeros(len(fake_news))))\n",
    "test_labels_fake = np.concatenate((np.zeros(len(real_news)), np.ones(len(fake_news))))\n",
    "test_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Test:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_dict = {'real': train_labels_real, 'fake': train_labels_fake}\n",
    "test_labels_dict = {'real': test_labels_real, 'fake': test_labels_fake}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating experiment sets \n",
    "\n",
    "We're generating a few sets of experiments with different varyingvalues of N-grams for each feature type and their combinations.  This looks like incomplete exhaustive search because we actually don't consider (at least so far) _all_ possible combinations that sрould be roughly 3^6 = 729 and then multiply by varios normalization techniques by various classifiers. \n",
    "\n",
    "We actually can do it yet I am not sure whether it'll make sense and maybe there's a better \"smart grid search\"  strategy when we first lok at some parameter combination, then find the best and then look at other possible variations around that point of parameter values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [cnvalues, wnvalues, fnvalues]\n",
    "num_feat_types = 3 \n",
    "ngram_feat_combinations = [[], [], []]\n",
    "\n",
    "# for a single type n-grams\n",
    "for j in range(num_feat_types): \n",
    "    for i in range(1,7):\n",
    "        ngram_feat_combinations[j].append([i])\n",
    "        for k in range(num_feat_types): \n",
    "            if k != j: \n",
    "                ngram_feat_combinations[k].append([0])\n",
    "        \n",
    "# extending with 1-1-1, 2-2-2,3-3-3,4-4-4,5-5-5,6-6-6 variants\n",
    "ngrams = [[x] for x in range(1,7)]\n",
    "\n",
    "for ngram_type in ngram_feat_combinations: \n",
    "    ngram_type.extend(ngrams)\n",
    "    \n",
    "# combinations of varying X-grams - uni-Y\n",
    "for l in range(num_feat_types):\n",
    "    for j in range(num_feat_types):\n",
    "        if j != l: \n",
    "            for i in range(1,7):\n",
    "                ngram_feat_combinations[l].append([i])\n",
    "                ngram_feat_combinations[j].append([1])    \n",
    "                for k in range(num_feat_types):                 \n",
    "                    if k != j and k != l: \n",
    "                        ngram_feat_combinations[k].append([0])       \n",
    "        \n",
    "len(ngram_feat_combinations[0])                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated experiments:  \n",
    "for i in range(10):     \n",
    "        for feat in ngram_feat_combinations:\n",
    "                print(feat[i*6 : i*6+6])\n",
    "        print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cnvalues=[4] #character n-grams\n",
    "wnvalues=[0] # word n-grams\n",
    "fnvalues=[0] # function words n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor: \n",
    "    \"\"\"Feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, cnvalues, wnvalues, fnvalues): \n",
    "        print('Feature extractor for ' + str(cnvalues[0]) +'-' + str(wnvalues[0]) + '-' + str(fnvalues[0]) + ' char-word-func N-gram combination')\n",
    "        self.cnvalues = cnvalues\n",
    "        self.wnvalues = wnvalues\n",
    "        self.fnvalues = fnvalues\n",
    "        \n",
    "        self.vectorizer = CountVectorizer(lowercase=False, min_df=2, tokenizer=lambda x: x.split('&%$')) #--> we can change this\n",
    "        \n",
    "    def fit_extract(self, train_texts):\n",
    "        train_features, dicOfFeatures = process_texts(train_texts, self.cnvalues, self.wnvalues, self.fnvalues)\n",
    "        train_data = self.vectorizer.fit_transform(train_features)\n",
    "        train_data = train_data.astype(float)\n",
    "        return train_data, dicOfFeatures\n",
    "    \n",
    "    def extract(self, test_texts): \n",
    "        test_features, dicOfFeaturesTest = process_texts(test_texts, self.cnvalues, self.wnvalues, self.fnvalues)\n",
    "        test_data = self.vectorizer.transform(test_features)\n",
    "        test_data = test_data.astype(float)\n",
    "        return test_data, dicOfFeaturesTest            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(texts, cnvalues, wnvalues, fnvalues):\n",
    "    \"\"\"Train feature extraction\"\"\"\n",
    "    print('Extracting features for ' + str(cnvalues[0]) +'-' + str(wnvalues[0]) + '-' + str(fnvalues[0]) + ' char-word-func N-gram combination')\n",
    "    features, dicOfFeatures = process_texts(texts, cnvalues, wnvalues, fnvalues)\n",
    "\n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=2, tokenizer=lambda x: x.split('&%$')) #--> we can change this\n",
    "    feature_mx = vectorizer.fit_transform(features)\n",
    "    feature_mx = feature_mx.astype(float)\n",
    "    return feature_mx, dicOfFeatures    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "\t Total training files (Real + Fake) :  638\n",
      "\t Vocabulary size of 638 files is :  55261\n",
      "\t Train shape: (638, 36272)\n"
     ]
    }
   ],
   "source": [
    "#Train feature extraction\n",
    "print('Extracting features')\n",
    "train_features, dicOfFeatures = process_texts(train_texts,cnvalues,wnvalues,fnvalues)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, min_df=2, tokenizer=lambda x: x.split('&%$')) #--> we can change this\n",
    "train_data = vectorizer.fit_transform(train_features)\n",
    "train_data = train_data.astype(float)\n",
    "#print('\\t', 'Labels for each document: ', len(train_labels))\n",
    "print('\\t', 'Total training files (Real + Fake) : ', len(train_texts))\n",
    "print('\\t', 'Vocabulary size of', len(train_texts), 'files is : ',len(dicOfFeatures))\n",
    "print ('\\t','Train shape:',train_data.shape)\n",
    "#print('\\t', 'class dictribution',Counter(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-98eb4159cc62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "train_data.shape(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction\n",
    "print('Extracting Test features')\n",
    "test_features,dicOfFeaturesTest = process_texts(test_texts,cnvalues,wnvalues,fnvalues)\n",
    "test_data = vectorizer.transform(test_features)\n",
    "test_data = test_data.astype(float)\n",
    "\n",
    "print('\\t', 'texts', len(test_texts))\n",
    "print('\\t', 'vocabulary size',len(dicOfFeaturesTest))\n",
    "\n",
    "print ('\\t','Test shape:',test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_frequency_threshold(feature_mx, N=5):\n",
    "    values=np.array(feature_mx.sum(axis=0)).ravel()\n",
    "    thresholdMask=(values >= N)*1\n",
    "    indices_zero = list(np.nonzero(thresholdMask == 0)[0])\n",
    "    all_cols = np.arange(feature_mx.shape[1])\n",
    "    cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "    return cols_to_keep \n",
    "\n",
    "#cols_to_keep = apply_frequency_threshold(feature_mx)\n",
    "#thresholded_train_feat_mx = train_feature_mx[:, cols_to_keep]\n",
    "#thresholded_test_feat_mx = test_feature_mx[:, cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5\n",
    "X=train_data\n",
    "values=np.array(X.sum(axis=0)).ravel()\n",
    "thresholdMask=(values >= N)*1\n",
    "indices_zero = list(np.nonzero(thresholdMask == 0)[0])\n",
    "all_cols = np.arange(X.shape[1])\n",
    "cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "train_data = X[:, cols_to_keep]\n",
    "#####\n",
    "scaled_train_data=train_data\n",
    "print('Train shape:',scaled_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbral de frecuencia, la N ya no se define\n",
    "Z=test_data\n",
    "all_cols = np.arange(Z.shape[1])\n",
    "cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "test_data = Z[:, cols_to_keep]\n",
    "scaled_test_data=test_data\n",
    "print('Test shape:',scaled_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible values: binary, logent, tfidf, norm, relat\n",
    "weighting_schemes = ['binary', 'logent', 'tfidf', 'norm', 'relat', 'none (tf)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import Scipy2Corpus, corpus2csc\n",
    "from gensim.models.logentropy_model import LogEntropyModel\n",
    "\n",
    "class LogEntWeightScheme: \n",
    "        \n",
    "    def __init__(self): \n",
    "        self.log_ent = LogEntropyModel()         \n",
    "        \n",
    "    def fit(self, scaled_train_data):\n",
    "        Xc = Scipy2Corpus(scaled_train_data)\n",
    "        self.log_ent = LogEntropyModel(Xc)\n",
    "        X = self.log_ent[Xc]\n",
    "        X = corpus2csc(X)\n",
    "        scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "        return scaled_train_data\n",
    "        \n",
    "    def fit_transform(self, scaled_train_data):    \n",
    "        Xc = Scipy2Corpus(scaled_train_data)\n",
    "        self.log_ent = LogEntropyModel(Xc)\n",
    "        X = self.log_ent[Xc]\n",
    "        X = corpus2csc(X)\n",
    "        scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "        return scaled_train_data \n",
    "\n",
    "    def transform(self, scaled_test_data):\n",
    "        Xtest = Scipy2Corpus(scaled_test_data)\n",
    "        X = self.log_ent[Xtest]\n",
    "        X = corpus2csc(X, scaled_train_data.shape[1])\n",
    "        scaled_test_data = sp.csc_matrix.transpose(X)\n",
    "        return scaled_test_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class RelativeWeightScheme:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s = 1.0 \n",
    "             \n",
    "    def fit(self, scaled_train_data):\n",
    "        self.s = float(scaled_train_data.sum(axis = 1))\n",
    "        scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/self.s))\n",
    "        return scaled_train_data\n",
    "\n",
    "    # identical to fit()\n",
    "    def fit_transform(self, scaled_train_data):\n",
    "        self.s = float(scaled_train_data.sum(axis = 1))\n",
    "        scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/self.s))\n",
    "        return scaled_train_data\n",
    "                \n",
    "    def transform(self, scaled_test_data):                      \n",
    "        scaled_test_data = coo_matrix(np.nan_to_num(scaled_test_data/self.s))\n",
    "        return scaled_test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoNothingWeightScheme:\n",
    "    \"\"\"Making it easier to comply with APIs for a do-nothing trasnformer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, scaled_train_data):\n",
    "        return scaled_train_data\n",
    "\n",
    "    def fit_transform(self, scaled_train_data):\n",
    "        return scaled_train_data\n",
    "\n",
    "    def transform(self, scaled_test_data):\n",
    "        return scaled_test_data\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighting_scheme(feature_weight_name): \n",
    "    \n",
    "    if feature_weight_name == 'binary':\n",
    "        print (\"feature_weight = binary\")\n",
    "        return preprocessing.Binarizer()\n",
    "\n",
    "    elif feature_weight_name == 'logent':\n",
    "        print (\"feature_weight = logent\")\n",
    "        return LogEntWeightScheme()\n",
    "        \n",
    "    elif feature_weight_name == 'tfidf':\n",
    "        print (\"feature_weight = tfidf\")\n",
    "        return TfidfTransformer()\n",
    "\n",
    "    elif feature_weight_name == 'norm':\n",
    "        # cannot normalize with l2 in a Pipeline\n",
    "        print (\"feature_weight = norm\")\n",
    "        return preprocessing.MaxAbsScaler()\n",
    "\n",
    "    elif feature_weight_name == 'relat':\n",
    "        print (\"feature_weight = relat\")\n",
    "        return RelativeWeightScheme()\n",
    "\n",
    "    else:\n",
    "        print (\"feature_weight = tf\")\n",
    "        return DoNothingWeightScheme()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weighing_scheme(scaled_train_data, test_data, feature_weight): \n",
    "    print(\"Applying weighting scheme\")\n",
    "\n",
    "    if feature_weight == 'binary':\n",
    "        scaled_train_data = preprocessing.Binarizer().transform(scaled_train_data)\n",
    "        scaled_test_data = preprocessing.Binarizer().transform(scaled_test_data)\n",
    "        print (\"feature_weight = binary\")\n",
    "\n",
    "    elif feature_weight == 'logent':\n",
    "        Xc = Scipy2Corpus(scaled_train_data)\n",
    "        log_ent = LogEntropyModel(Xc)\n",
    "        X = log_ent[Xc]\n",
    "        X = corpus2csc(X)\n",
    "        scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "\n",
    "        Xtest = Scipy2Corpus(scaled_test_data)\n",
    "        X = log_ent[Xtest]\n",
    "        X = corpus2csc(X, scaled_train_data.shape[1])\n",
    "        scaled_test_data = sp.csc_matrix.transpose(X)\n",
    "        print (\"feature_weight = logent\")\n",
    "\n",
    "    elif feature_weight == 'tfidf':\n",
    "        transformer = TfidfTransformer()\n",
    "        scaled_train_data = transformer.fit_transform(scaled_train_data)\n",
    "        scaled_test_data = transformer.transform(scaled_test_data)\n",
    "        print (\"feature_weight = tfidf\")\n",
    "\n",
    "    elif feature_weight=='norm':\n",
    "        #scaled_train_data = preprocessing.normalize(scaled_train_data, norm='l2')\n",
    "        #Scaling data\n",
    "        scaled_train_data = preprocessing.normalize(scaled_train_data, norm='l2')\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(scaled_train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(scaled_test_data)\n",
    "        print (\"feature_weight = norm\")\n",
    "\n",
    "    elif feature_weight=='relat':\n",
    "        s = scaled_train_data.sum(axis = 1)\n",
    "        scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/s))\n",
    "\n",
    "        #s = scaled_test_data.sum(axis = 1)\n",
    "        scaled_test_data = coo_matrix(np.nan_to_num(scaled_test_data/s))\n",
    "        print (\"feature_weight = relat\")\n",
    "\n",
    "    else:\n",
    "        print (\"feature_weight = tf\")\n",
    "\n",
    "    return scaled_train_data, scaled_test_data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ('only frecuency:',test_data)\n",
    "feature_weight='logent' # possible values: binary, logent, tfidf, norm, relat\n",
    "print ('Train:',scaled_train_data.shape)\n",
    "print ('Test:',scaled_test_data.shape)\n",
    "\n",
    "if feature_weight == 'binary':\n",
    "    scaled_train_data = preprocessing.Binarizer().fit_transform(scaled_train_data)\n",
    "    scaled_test_data = preprocessing.Binarizer().fit_transform(scaled_test_data)\n",
    "    print (\"feature_weight = binary\")\n",
    "    \n",
    "elif feature_weight == 'logent':\n",
    "    Xc = Scipy2Corpus(scaled_train_data)\n",
    "    log_ent = LogEntropyModel(Xc)\n",
    "    X = log_ent[Xc]\n",
    "    X = corpus2csc(X)\n",
    "    scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "    \n",
    "    Xtest = Scipy2Corpus(scaled_test_data)\n",
    "    X = log_ent[Xtest]\n",
    "    X = corpus2csc(X, scaled_train_data.shape[1])\n",
    "    scaled_test_data = sp.csc_matrix.transpose(X)\n",
    "    print (\"feature_weight = logent\")\n",
    "    \n",
    "elif feature_weight == 'tfidf':\n",
    "    transformer = TfidfTransformer()\n",
    "    scaled_train_data = transformer.fit_transform(scaled_train_data)\n",
    "    scaled_test_data = transformer.transform(scaled_test_data)\n",
    "    print (\"feature_weight = tfidf\")\n",
    "    \n",
    "elif feature_weight=='norm':\n",
    "    #scaled_train_data = preprocessing.normalize(scaled_train_data, norm='l2')\n",
    "    #Scaling data\n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    scaled_train_data = max_abs_scaler.fit_transform(scaled_train_data)\n",
    "    scaled_test_data = max_abs_scaler.transform(scaled_test_data)\n",
    "    print (\"feature_weight = norm\")\n",
    "    \n",
    "elif feature_weight=='relat':\n",
    "    s = scaled_train_data.sum(axis = 1)\n",
    "    scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/s))\n",
    "\n",
    "\n",
    "    #s = scaled_test_data.sum(axis = 1)\n",
    "    scaled_test_data = coo_matrix(np.nan_to_num(scaled_test_data/s))\n",
    "    print (\"feature_weight = relat\")\n",
    "    \n",
    "else:\n",
    "    print (\"feature_weight = tf\")\n",
    "    \n",
    "print ('Train:',scaled_train_data.shape)\n",
    "print ('Test:',scaled_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Process - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from IPython.display import display\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "def check_model_fit(clf, X_test, y_test):\n",
    "    # Print overall test-set accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    #print('total accuracy = {:.1f}%'.format(acc))\n",
    "\n",
    "    p = precision_score(y_test, y_pred)\n",
    "    r = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "       \n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)     \n",
    "    \n",
    "    y_pred_scores = clf.predict_proba(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_scores[:, 1])        \n",
    "    \n",
    "    # Print confusion matrix\n",
    "    #cmat = confusion_matrix(y_test, y_pred)\n",
    "    #cols = pd.MultiIndex.from_tuples([('predictions', 0), ('predictions', 1)])\n",
    "    #indx = pd.MultiIndex.from_tuples([('actual', 0), ('actual', 1)])\n",
    "    #display(pd.DataFrame(cmat, columns=cols, index=indx))\n",
    "    #print()\n",
    "    \n",
    "    # Print test-set accuracy grouped by the target variable \n",
    "    #print('percent accuracy score per class:')\n",
    "    #cmat = confusion_matrix(y_test, y_pred)\n",
    "    #scores = cmat.diagonal() / cmat.sum(axis=1) * 100\n",
    "    # specificity: \n",
    "    #irrelevant_acc = scores[0]\n",
    "    # sensetivity:  \n",
    "    #relevant_acc = scores[1]\n",
    "    #balanced_acc = (relevant_acc + irrelevant_acc)/2.0\n",
    "    #print('left = 0 : {:.2f}%'.format(scores[0]))\n",
    "    #print('left = 1 : {:.2f}%'.format(scores[1]))\n",
    "    #print()\n",
    "    \n",
    "    # Plot decision regions\n",
    "    #fig = plt.figure(figsize=(8, 8))\n",
    "    #N_samples = 200\n",
    "    #X, y = X_test[:N_samples], y_test[:N_samples]\n",
    "    #plot_decision_regions(X, y, clf=clf)\n",
    "    \n",
    "    #plt.xlabel('satisfaction_level')\n",
    "    #plt.ylabel('last_evaluation')\n",
    "    #plt.legend(loc='upper left')\n",
    "    \n",
    "    results = {'accuracy': acc, \n",
    "               'balanced_accuracy': balanced_acc, \n",
    "               'precision': p, \n",
    "               'recall': r, \n",
    "               'f1': f1, \n",
    "               'roc_auc': roc_auc}\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function\n",
    "originalclass=[]\n",
    "predictedclass=[]\n",
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return f1_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Classifier')\n",
    "    \n",
    "# Applying classification algorithms\n",
    "clf=LinearSVC(C=0.01,class_weight='balanced', random_state=85)\n",
    "clfSVC=SVC(C=0.01, kernel='linear',class_weight='balanced')\n",
    "clfMnb=MultinomialNB()\n",
    "clfBnb=BernoulliNB()\n",
    "clfLG=LogisticRegression(solver='lbfgs', tol=0.001, C=0.01,class_weight='balanced')\n",
    "\n",
    "clf.fit(scaled_train_data, train_labels)\n",
    "nested_score = cross_val_score(clf, X=scaled_train_data, y=train_labels, cv=10, scoring=make_scorer(classification_report_with_f1_score))\n",
    "#cvScoreLinearSVC=cross_val_score(clf, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "print('10-Fold Cross-validation Linear SVC',nested_score.mean())\n",
    "\n",
    "cvScoreLG=cross_val_score(clfLG, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "\n",
    "cvScoreMnb=cross_val_score(clfMnb, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "\n",
    "cvScoreBnb=cross_val_score(clfBnb, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Bernoulli Naive Bayes',cvScoreBnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZ:  I don't think we need to balance the classes: \n",
    "# first, the data is not so heavily imbalanced \n",
    "# second, we loose the class distribution information  \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "\n",
    "clfLG = LogisticRegression(solver='lbfgs', tol=0.001, C=0.01)\n",
    "clfLinSVM = LinearSVC(C=0.01, random_state=85)\n",
    "clfSVC = SVC(C=0.01, kernel='linear')\n",
    "clfDT = DecisionTreeClassifier(random_state=0)\n",
    "clfRFC = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clfMnb = MultinomialNB()\n",
    "clfBnb = BernoulliNB()\n",
    "clfAB = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0,  random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [clfLG, clfLinSVM, clfSVC, clfDT, clfRFC, clfMnb, clfBnb,  clfAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def cv(clf, train_X, train_y, scoring=['accuracy', 'balanced_accuracy' ,'precision', 'recall', 'f1', 'roc_auc']): \n",
    "    cv = cross_validate(clf, train_X, train_y, cv=5, return_estimator=True, scoring=scoring)\n",
    "    \n",
    "    output = {}\n",
    "    for s in scoring: \n",
    "        m = np.mean(cv['test_' + s])\n",
    "        v = np.var(cv['test_' + s])\n",
    "        output[s] ={'mean': m, 'var': v}      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes running a set of experiments  and outputs the values  \n",
    "\n",
    "* Doesn't include reading in texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N-gram-feat-comb</th>\n",
       "      <th>total_feat_num</th>\n",
       "      <th>weight scheme</th>\n",
       "      <th>classifier</th>\n",
       "      <th>cv-acc</th>\n",
       "      <th>cv-balanced_acc</th>\n",
       "      <th>cv-roc</th>\n",
       "      <th>cv-f1-fake</th>\n",
       "      <th>acc</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>roc</th>\n",
       "      <th>f1_fake</th>\n",
       "      <th>f1_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [N-gram-feat-comb, total_feat_num, weight scheme, classifier, cv-acc, cv-balanced_acc, cv-roc, cv-f1-fake, acc, balanced_acc, roc, f1_fake, f1_real]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "report = pd.DataFrame(columns=['N-gram-feat-comb', 'total_feat_num', 'weight scheme', 'classifier', 'cv-acc', 'cv-balanced_acc','cv-roc', 'cv-f1-fake', 'acc', 'balanced_acc', 'roc', 'f1_fake', 'f1_real'])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def end_to_end(train_texts, train_labels_dict, test_texts, test_labels_dict, \n",
    "             ngram_feat_sets, weight_schemes, clfs, \n",
    "             expLog_df, name_prefix=''):  \n",
    "    \n",
    "    all_experiments = {}\n",
    "    \n",
    "    # for each combination of ngram feature types\n",
    "    for i in range(len(ngram_feat_sets[0])):         \n",
    "        cnvalues = ngram_feat_sets[0][i]\n",
    "        wnvalues = ngram_feat_sets[1][i] \n",
    "        fnvalues = ngram_feat_sets[2][i] \n",
    "        ngram_feat_type_set_str = str(cnvalues[0]) +'-' + str(wnvalues[0]) + '-' + str(fnvalues[0]) + ' char-word-func'\n",
    "        \n",
    "        feat_extractor = FeatureExtractor(cnvalues, wnvalues, fnvalues)        \n",
    "        train_feature_mx, _ = feat_extractor.fit_extract(train_texts)\n",
    "        test_feature_mx, _ = feat_extractor.extract(test_texts) \n",
    "        print('train shape: ', train_feature_mx.shape, 'test shape:', test_feature_mx.shape)\n",
    "        \n",
    "        #applying the frequency threshold\n",
    "        cols_to_keep = apply_frequency_threshold(train_feature_mx)\n",
    "        thresholded_train_feat_mx = train_feature_mx[:, cols_to_keep]\n",
    "        thresholded_test_feat_mx = test_feature_mx[:, cols_to_keep]\n",
    "        \n",
    "        #for each weighting scheme, for each classifier, create a pipeline\n",
    "        pipelines = []\n",
    "        \n",
    "        all_experiments[ngram_feat_type_set_str] = {}\n",
    "        for weight_scheme_name in weight_schemes: \n",
    "            all_experiments[ngram_feat_type_set_str][weight_scheme_name] = {}\n",
    "            weight_scheme = get_weighting_scheme(weight_scheme_name)\n",
    "            for clf in clfs: \n",
    "                clf_name = type(clf).__name__\n",
    "                print('Classifier: ' + clf_name)\n",
    "                all_experiments[ngram_feat_type_set_str][weight_scheme_name][clf_name] = {}\n",
    "                ppl = Pipeline([('scaler', weight_scheme), ('clf', clf)])\n",
    "\n",
    "                cv_scores_by_labeling = {}\n",
    "                test_scores_by_labeling = {}\n",
    "                for key in train_labels_dict:\n",
    "                    print(key + \" run...\")\n",
    "                    train_X = thresholded_train_feat_mx.copy()\n",
    "                    train_X_cv = thresholded_train_feat_mx.copy()\n",
    "                    test_X = thresholded_test_feat_mx.copy() \n",
    "\n",
    "                    train_y = train_labels_dict[key]\n",
    "                    test_y = test_labels_dict[key]\n",
    "                    \n",
    "                    # cross-validation run \n",
    "                    # scoring=['accuracy', 'balanced_accuracy' ,'precision', 'recall', 'f1', 'roc_auc']\n",
    "                    cv_scores = cv(ppl, train_X_cv, train_y)\n",
    "                    cv_scores_by_labeling[key] = cv_scores\n",
    "                    \n",
    "                    # test run \n",
    "                    ppl.fit(train_X, train_y)\n",
    "                    #acc, balanced_acc, p, r, f1, roc_auc\n",
    "                    test_scores = check_model_fit(ppl, test_X, test_y)\n",
    "                    test_scores_by_labeling[key] = test_scores\n",
    "                \n",
    "                all_experiments[ngram_feat_type_set_str][weight_scheme_name][clf_name]['cv'] = cv_scores_by_labeling\n",
    "                all_experiments[ngram_feat_type_set_str][weight_scheme_name][clf_name]['test'] = test_scores_by_labeling\n",
    "                                \n",
    "                # record experiments to the log \n",
    "                #['N-gram-feat-comb', 'total_feat_num', 'weight scheme', 'classifier', \n",
    "                # 'cv-acc', 'cv-balanced_acc','cv-roc', 'cv-f1-fake', \n",
    "                # 'acc', 'balanced_acc', 'roc', 'f1_fake', 'f1_real']\n",
    "                expLog_df.loc[len(expLog_df)] = [name_prefix + ngram_feat_type_set_str, thresholded_train_feat_mx.shape[1], weight_scheme_name, clf_name,\n",
    "                                           np.round(cv_scores_by_labeling['fake']['accuracy']['mean'], 2), np.round(cv_scores_by_labeling['fake']['balanced_accuracy']['mean'], 2), np.round(cv_scores_by_labeling['fake']['roc_auc']['mean'], 2), np.round(cv_scores_by_labeling['fake']['f1']['mean'], 2), \n",
    "                                           np.round(test_scores_by_labeling['fake']['accuracy'], 2), np.round(test_scores_by_labeling['fake']['balanced_accuracy'], 2), np.round(test_scores_by_labeling['fake']['roc_auc'], 2), np.round(test_scores_by_labeling['fake']['f1'], 2), np.round(test_scores_by_labeling['real']['f1'], 2)]\n",
    "                \n",
    "                expLog_df\n",
    "                \n",
    "    return all_experiments, expLog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 110) test shape: (262, 110)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 2029) test shape: (262, 2029)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 11666) test shape: (262, 11666)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 36272) test shape: (262, 36272)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 73096) test shape: (262, 73096)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 108412) test shape: (262, 108412)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 8347) test shape: (262, 8347)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 31621) test shape: (262, 31621)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 38619) test shape: (262, 38619)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 35834) test shape: (262, 35834)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 31807) test shape: (262, 31807)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 28389) test shape: (262, 28389)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 310) test shape: (262, 310)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 6968) test shape: (262, 6968)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 19001) test shape: (262, 19001)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 21822) test shape: (262, 21822)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 19345) test shape: (262, 19345)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 17413) test shape: (262, 17413)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/z0034bv/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (638, 8767) test shape: (262, 8767)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 40618) test shape: (262, 40618)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 69286) test shape: (262, 69286)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 93928) test shape: (262, 93928)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 124248) test shape: (262, 124248)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 154214) test shape: (262, 154214)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 8457) test shape: (262, 8457)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 10376) test shape: (262, 10376)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 20013) test shape: (262, 20013)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 44619) test shape: (262, 44619)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 81443) test shape: (262, 81443)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 116759) test shape: (262, 116759)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 420) test shape: (262, 420)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 2339) test shape: (262, 2339)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 11976) test shape: (262, 11976)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 36582) test shape: (262, 36582)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 73406) test shape: (262, 73406)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 108722) test shape: (262, 108722)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 8457) test shape: (262, 8457)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 31731) test shape: (262, 31731)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 38729) test shape: (262, 38729)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 35944) test shape: (262, 35944)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 31917) test shape: (262, 31917)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 28499) test shape: (262, 28499)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 8657) test shape: (262, 8657)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 31931) test shape: (262, 31931)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 38929) test shape: (262, 38929)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 36144) test shape: (262, 36144)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 32117) test shape: (262, 32117)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 28699) test shape: (262, 28699)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 420) test shape: (262, 420)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 7078) test shape: (262, 7078)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 19111) test shape: (262, 19111)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 21932) test shape: (262, 21932)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 19455) test shape: (262, 19455)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 17523) test shape: (262, 17523)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 8657) test shape: (262, 8657)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 15315) test shape: (262, 15315)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 27348) test shape: (262, 27348)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 30169) test shape: (262, 30169)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 27692) test shape: (262, 27692)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n",
      "train shape:  (638, 25760) test shape: (262, 25760)\n",
      "feature_weight = binary\n",
      "Classifier: LogisticRegression\n",
      "realrun\n",
      "fakerun\n"
     ]
    }
   ],
   "source": [
    "# small experiment \n",
    "\n",
    "my_expLog = pd.DataFrame(columns=['N-gram-feat-comb', 'total_feat_num', 'weight scheme', 'classifier', 'cv-acc', 'cv-balanced_acc','cv-roc', 'cv-f1-fake', 'acc', 'balanced_acc', 'roc', 'f1_fake', 'f1_real'])\n",
    "\n",
    "#ngram_feat_combinations\n",
    "\n",
    "weighting_schemes = ['binary', 'logent', 'tfidf', 'norm', 'relat', 'none (tf)']\n",
    "#my_weight_schemes = ['binary']\n",
    "#my_clfs = [clfLG]\n",
    "clfs = [clfLG, clfLinSVM, clfSVC, clfDT, clfRFC, clfMnb, clfBnb,  clfAB]\n",
    "\n",
    "all_results, log = end_to_end(train_texts, train_labels_dict, test_texts, test_labels_dict, \n",
    "             ngram_feat_combinations, my_weight_schemes, my_clfs, \n",
    "             my_expLog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N-gram-feat-comb</th>\n",
       "      <th>total_feat_num</th>\n",
       "      <th>weight scheme</th>\n",
       "      <th>classifier</th>\n",
       "      <th>cv-acc</th>\n",
       "      <th>cv-balanced_acc</th>\n",
       "      <th>cv-roc</th>\n",
       "      <th>cv-f1-fake</th>\n",
       "      <th>acc</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>roc</th>\n",
       "      <th>f1_fake</th>\n",
       "      <th>f1_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my-[1]-char+[0]-word+[0]-func</td>\n",
       "      <td>101</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my-[2]-char+[0]-word+[0]-func</td>\n",
       "      <td>1555</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my-[3]-char+[0]-word+[0]-func</td>\n",
       "      <td>7927</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my-[4]-char+[0]-word+[0]-func</td>\n",
       "      <td>20681</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my-[5]-char+[0]-word+[0]-func</td>\n",
       "      <td>31926</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my-[6]-char+[0]-word+[0]-func</td>\n",
       "      <td>37105</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my-[0]-char+[1]-word+[0]-func</td>\n",
       "      <td>4020</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my-[0]-char+[2]-word+[0]-func</td>\n",
       "      <td>6690</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>my-[0]-char+[3]-word+[0]-func</td>\n",
       "      <td>2860</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my-[0]-char+[4]-word+[0]-func</td>\n",
       "      <td>847</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>my-[0]-char+[5]-word+[0]-func</td>\n",
       "      <td>248</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>my-[0]-char+[6]-word+[0]-func</td>\n",
       "      <td>85</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>my-[0]-char+[0]-word+[1]-func</td>\n",
       "      <td>293</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>my-[0]-char+[0]-word+[2]-func</td>\n",
       "      <td>3203</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my-[0]-char+[0]-word+[3]-func</td>\n",
       "      <td>3790</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>my-[0]-char+[0]-word+[4]-func</td>\n",
       "      <td>1020</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>my-[0]-char+[0]-word+[5]-func</td>\n",
       "      <td>136</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my-[0]-char+[0]-word+[6]-func</td>\n",
       "      <td>18</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>my-[1]-char+[1]-word+[1]-func</td>\n",
       "      <td>4414</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>my-[2]-char+[2]-word+[2]-func</td>\n",
       "      <td>11448</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>my-[3]-char+[3]-word+[3]-func</td>\n",
       "      <td>14577</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>my-[4]-char+[4]-word+[4]-func</td>\n",
       "      <td>22548</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>my-[5]-char+[5]-word+[5]-func</td>\n",
       "      <td>32310</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>my-[6]-char+[6]-word+[6]-func</td>\n",
       "      <td>37208</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>my-[1]-char+[1]-word+[0]-func</td>\n",
       "      <td>4121</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>my-[2]-char+[1]-word+[0]-func</td>\n",
       "      <td>5575</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>my-[3]-char+[1]-word+[0]-func</td>\n",
       "      <td>11947</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>my-[4]-char+[1]-word+[0]-func</td>\n",
       "      <td>24701</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>my-[5]-char+[1]-word+[0]-func</td>\n",
       "      <td>35946</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>my-[6]-char+[1]-word+[0]-func</td>\n",
       "      <td>41125</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>my-[1]-char+[0]-word+[1]-func</td>\n",
       "      <td>394</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>my-[2]-char+[0]-word+[1]-func</td>\n",
       "      <td>1848</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>my-[3]-char+[0]-word+[1]-func</td>\n",
       "      <td>8220</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>my-[4]-char+[0]-word+[1]-func</td>\n",
       "      <td>20974</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>my-[5]-char+[0]-word+[1]-func</td>\n",
       "      <td>32219</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>my-[6]-char+[0]-word+[1]-func</td>\n",
       "      <td>37398</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>my-[1]-char+[1]-word+[0]-func</td>\n",
       "      <td>4121</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>my-[1]-char+[2]-word+[0]-func</td>\n",
       "      <td>6791</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>my-[1]-char+[3]-word+[0]-func</td>\n",
       "      <td>2961</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>my-[1]-char+[4]-word+[0]-func</td>\n",
       "      <td>948</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>my-[1]-char+[5]-word+[0]-func</td>\n",
       "      <td>349</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>my-[1]-char+[6]-word+[0]-func</td>\n",
       "      <td>186</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>my-[0]-char+[1]-word+[1]-func</td>\n",
       "      <td>4313</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>my-[0]-char+[2]-word+[1]-func</td>\n",
       "      <td>6983</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>my-[0]-char+[3]-word+[1]-func</td>\n",
       "      <td>3153</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>my-[0]-char+[4]-word+[1]-func</td>\n",
       "      <td>1140</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>my-[0]-char+[5]-word+[1]-func</td>\n",
       "      <td>541</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>my-[0]-char+[6]-word+[1]-func</td>\n",
       "      <td>378</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>my-[1]-char+[0]-word+[1]-func</td>\n",
       "      <td>394</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>my-[1]-char+[0]-word+[2]-func</td>\n",
       "      <td>3304</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>my-[1]-char+[0]-word+[3]-func</td>\n",
       "      <td>3891</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>my-[1]-char+[0]-word+[4]-func</td>\n",
       "      <td>1121</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>my-[1]-char+[0]-word+[5]-func</td>\n",
       "      <td>237</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>my-[1]-char+[0]-word+[6]-func</td>\n",
       "      <td>119</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>my-[0]-char+[1]-word+[1]-func</td>\n",
       "      <td>4313</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>my-[0]-char+[1]-word+[2]-func</td>\n",
       "      <td>7223</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>my-[0]-char+[1]-word+[3]-func</td>\n",
       "      <td>7810</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>my-[0]-char+[1]-word+[4]-func</td>\n",
       "      <td>5040</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>my-[0]-char+[1]-word+[5]-func</td>\n",
       "      <td>4156</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>my-[0]-char+[1]-word+[6]-func</td>\n",
       "      <td>4038</td>\n",
       "      <td>binary</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 N-gram-feat-comb total_feat_num weight scheme  \\\n",
       "0   my-[1]-char+[0]-word+[0]-func            101        binary   \n",
       "1   my-[2]-char+[0]-word+[0]-func           1555        binary   \n",
       "2   my-[3]-char+[0]-word+[0]-func           7927        binary   \n",
       "3   my-[4]-char+[0]-word+[0]-func          20681        binary   \n",
       "4   my-[5]-char+[0]-word+[0]-func          31926        binary   \n",
       "5   my-[6]-char+[0]-word+[0]-func          37105        binary   \n",
       "6   my-[0]-char+[1]-word+[0]-func           4020        binary   \n",
       "7   my-[0]-char+[2]-word+[0]-func           6690        binary   \n",
       "8   my-[0]-char+[3]-word+[0]-func           2860        binary   \n",
       "9   my-[0]-char+[4]-word+[0]-func            847        binary   \n",
       "10  my-[0]-char+[5]-word+[0]-func            248        binary   \n",
       "11  my-[0]-char+[6]-word+[0]-func             85        binary   \n",
       "12  my-[0]-char+[0]-word+[1]-func            293        binary   \n",
       "13  my-[0]-char+[0]-word+[2]-func           3203        binary   \n",
       "14  my-[0]-char+[0]-word+[3]-func           3790        binary   \n",
       "15  my-[0]-char+[0]-word+[4]-func           1020        binary   \n",
       "16  my-[0]-char+[0]-word+[5]-func            136        binary   \n",
       "17  my-[0]-char+[0]-word+[6]-func             18        binary   \n",
       "18  my-[1]-char+[1]-word+[1]-func           4414        binary   \n",
       "19  my-[2]-char+[2]-word+[2]-func          11448        binary   \n",
       "20  my-[3]-char+[3]-word+[3]-func          14577        binary   \n",
       "21  my-[4]-char+[4]-word+[4]-func          22548        binary   \n",
       "22  my-[5]-char+[5]-word+[5]-func          32310        binary   \n",
       "23  my-[6]-char+[6]-word+[6]-func          37208        binary   \n",
       "24  my-[1]-char+[1]-word+[0]-func           4121        binary   \n",
       "25  my-[2]-char+[1]-word+[0]-func           5575        binary   \n",
       "26  my-[3]-char+[1]-word+[0]-func          11947        binary   \n",
       "27  my-[4]-char+[1]-word+[0]-func          24701        binary   \n",
       "28  my-[5]-char+[1]-word+[0]-func          35946        binary   \n",
       "29  my-[6]-char+[1]-word+[0]-func          41125        binary   \n",
       "30  my-[1]-char+[0]-word+[1]-func            394        binary   \n",
       "31  my-[2]-char+[0]-word+[1]-func           1848        binary   \n",
       "32  my-[3]-char+[0]-word+[1]-func           8220        binary   \n",
       "33  my-[4]-char+[0]-word+[1]-func          20974        binary   \n",
       "34  my-[5]-char+[0]-word+[1]-func          32219        binary   \n",
       "35  my-[6]-char+[0]-word+[1]-func          37398        binary   \n",
       "36  my-[1]-char+[1]-word+[0]-func           4121        binary   \n",
       "37  my-[1]-char+[2]-word+[0]-func           6791        binary   \n",
       "38  my-[1]-char+[3]-word+[0]-func           2961        binary   \n",
       "39  my-[1]-char+[4]-word+[0]-func            948        binary   \n",
       "40  my-[1]-char+[5]-word+[0]-func            349        binary   \n",
       "41  my-[1]-char+[6]-word+[0]-func            186        binary   \n",
       "42  my-[0]-char+[1]-word+[1]-func           4313        binary   \n",
       "43  my-[0]-char+[2]-word+[1]-func           6983        binary   \n",
       "44  my-[0]-char+[3]-word+[1]-func           3153        binary   \n",
       "45  my-[0]-char+[4]-word+[1]-func           1140        binary   \n",
       "46  my-[0]-char+[5]-word+[1]-func            541        binary   \n",
       "47  my-[0]-char+[6]-word+[1]-func            378        binary   \n",
       "48  my-[1]-char+[0]-word+[1]-func            394        binary   \n",
       "49  my-[1]-char+[0]-word+[2]-func           3304        binary   \n",
       "50  my-[1]-char+[0]-word+[3]-func           3891        binary   \n",
       "51  my-[1]-char+[0]-word+[4]-func           1121        binary   \n",
       "52  my-[1]-char+[0]-word+[5]-func            237        binary   \n",
       "53  my-[1]-char+[0]-word+[6]-func            119        binary   \n",
       "54  my-[0]-char+[1]-word+[1]-func           4313        binary   \n",
       "55  my-[0]-char+[1]-word+[2]-func           7223        binary   \n",
       "56  my-[0]-char+[1]-word+[3]-func           7810        binary   \n",
       "57  my-[0]-char+[1]-word+[4]-func           5040        binary   \n",
       "58  my-[0]-char+[1]-word+[5]-func           4156        binary   \n",
       "59  my-[0]-char+[1]-word+[6]-func           4038        binary   \n",
       "\n",
       "            classifier  cv-acc  cv-balanced_acc  cv-roc  cv-f1-fake   acc  \\\n",
       "0   LogisticRegression    0.66             0.64    0.79        0.52  0.75   \n",
       "1   LogisticRegression    0.79             0.79    0.89        0.78  0.85   \n",
       "2   LogisticRegression    0.68             0.68    0.81        0.64  0.82   \n",
       "3   LogisticRegression    0.66             0.65    0.75        0.60  0.79   \n",
       "4   LogisticRegression    0.64             0.64    0.72        0.59  0.75   \n",
       "5   LogisticRegression    0.63             0.63    0.70        0.58  0.74   \n",
       "6   LogisticRegression    0.65             0.64    0.73        0.57  0.74   \n",
       "7   LogisticRegression    0.64             0.63    0.68        0.51  0.67   \n",
       "8   LogisticRegression    0.62             0.59    0.68        0.35  0.61   \n",
       "9   LogisticRegression    0.59             0.55    0.65        0.18  0.58   \n",
       "10  LogisticRegression    0.55             0.50    0.60        0.00  0.57   \n",
       "11  LogisticRegression    0.55             0.50    0.55        0.00  0.57   \n",
       "12  LogisticRegression    0.64             0.62    0.68        0.48  0.66   \n",
       "13  LogisticRegression    0.63             0.61    0.65        0.48  0.68   \n",
       "14  LogisticRegression    0.61             0.57    0.62        0.33  0.61   \n",
       "15  LogisticRegression    0.56             0.51    0.58        0.06  0.58   \n",
       "16  LogisticRegression    0.55             0.50    0.55        0.00  0.57   \n",
       "17  LogisticRegression    0.55             0.50    0.52        0.00  0.57   \n",
       "18  LogisticRegression    0.70             0.69    0.82        0.64  0.77   \n",
       "19  LogisticRegression    0.78             0.78    0.87        0.75  0.86   \n",
       "20  LogisticRegression    0.70             0.69    0.80        0.65  0.82   \n",
       "21  LogisticRegression    0.66             0.65    0.75        0.60  0.78   \n",
       "22  LogisticRegression    0.64             0.63    0.72        0.59  0.74   \n",
       "23  LogisticRegression    0.63             0.63    0.70        0.58  0.75   \n",
       "24  LogisticRegression    0.71             0.70    0.84        0.65  0.81   \n",
       "25  LogisticRegression    0.79             0.79    0.89        0.77  0.86   \n",
       "26  LogisticRegression    0.68             0.68    0.80        0.64  0.82   \n",
       "27  LogisticRegression    0.65             0.65    0.75        0.60  0.79   \n",
       "28  LogisticRegression    0.65             0.64    0.73        0.59  0.75   \n",
       "29  LogisticRegression    0.65             0.64    0.72        0.60  0.76   \n",
       "30  LogisticRegression    0.71             0.70    0.84        0.62  0.80   \n",
       "31  LogisticRegression    0.81             0.81    0.90        0.79  0.84   \n",
       "32  LogisticRegression    0.70             0.69    0.81        0.66  0.82   \n",
       "33  LogisticRegression    0.66             0.65    0.75        0.60  0.78   \n",
       "34  LogisticRegression    0.65             0.64    0.73        0.59  0.74   \n",
       "35  LogisticRegression    0.63             0.63    0.71        0.59  0.74   \n",
       "36  LogisticRegression    0.71             0.70    0.84        0.65  0.81   \n",
       "37  LogisticRegression    0.70             0.69    0.82        0.62  0.82   \n",
       "38  LogisticRegression    0.71             0.69    0.84        0.59  0.77   \n",
       "39  LogisticRegression    0.68             0.66    0.82        0.55  0.76   \n",
       "40  LogisticRegression    0.67             0.65    0.80        0.53  0.76   \n",
       "41  LogisticRegression    0.66             0.64    0.79        0.52  0.75   \n",
       "42  LogisticRegression    0.66             0.65    0.73        0.59  0.72   \n",
       "43  LogisticRegression    0.66             0.65    0.70        0.57  0.69   \n",
       "44  LogisticRegression    0.65             0.63    0.71        0.52  0.68   \n",
       "45  LogisticRegression    0.65             0.63    0.71        0.50  0.66   \n",
       "46  LogisticRegression    0.65             0.63    0.70        0.50  0.66   \n",
       "47  LogisticRegression    0.65             0.62    0.69        0.49  0.66   \n",
       "48  LogisticRegression    0.71             0.70    0.84        0.62  0.80   \n",
       "49  LogisticRegression    0.69             0.67    0.80        0.59  0.78   \n",
       "50  LogisticRegression    0.70             0.68    0.81        0.59  0.74   \n",
       "51  LogisticRegression    0.68             0.66    0.80        0.54  0.76   \n",
       "52  LogisticRegression    0.67             0.65    0.79        0.53  0.75   \n",
       "53  LogisticRegression    0.66             0.64    0.79        0.52  0.75   \n",
       "54  LogisticRegression    0.66             0.65    0.73        0.59  0.72   \n",
       "55  LogisticRegression    0.65             0.64    0.71        0.58  0.71   \n",
       "56  LogisticRegression    0.66             0.65    0.72        0.58  0.72   \n",
       "57  LogisticRegression    0.66             0.65    0.73        0.58  0.73   \n",
       "58  LogisticRegression    0.65             0.64    0.73        0.57  0.74   \n",
       "59  LogisticRegression    0.65             0.64    0.73        0.57  0.74   \n",
       "\n",
       "    balanced_acc   roc  f1_fake  f1_real  \n",
       "0           0.72  0.85     0.65     0.80  \n",
       "1           0.84  0.94     0.81     0.88  \n",
       "2           0.80  0.90     0.77     0.85  \n",
       "3           0.76  0.85     0.71     0.83  \n",
       "4           0.72  0.83     0.64     0.81  \n",
       "5           0.71  0.81     0.63     0.80  \n",
       "6           0.72  0.79     0.63     0.80  \n",
       "7           0.63  0.72     0.48     0.76  \n",
       "8           0.56  0.65     0.29     0.73  \n",
       "9           0.52  0.59     0.08     0.73  \n",
       "10          0.50  0.59     0.00     0.73  \n",
       "11          0.50  0.55     0.00     0.73  \n",
       "12          0.63  0.68     0.49     0.75  \n",
       "13          0.64  0.68     0.51     0.76  \n",
       "14          0.55  0.65     0.28     0.73  \n",
       "15          0.51  0.61     0.05     0.73  \n",
       "16          0.50  0.53     0.00     0.73  \n",
       "17          0.50  0.52     0.00     0.73  \n",
       "18          0.76  0.87     0.70     0.82  \n",
       "19          0.84  0.93     0.81     0.89  \n",
       "20          0.80  0.90     0.77     0.85  \n",
       "21          0.76  0.85     0.70     0.83  \n",
       "22          0.72  0.83     0.63     0.80  \n",
       "23          0.72  0.81     0.64     0.81  \n",
       "24          0.80  0.89     0.76     0.85  \n",
       "25          0.84  0.94     0.82     0.88  \n",
       "26          0.81  0.90     0.77     0.86  \n",
       "27          0.77  0.85     0.71     0.84  \n",
       "28          0.72  0.83     0.65     0.81  \n",
       "29          0.73  0.82     0.64     0.81  \n",
       "30          0.79  0.86     0.75     0.84  \n",
       "31          0.83  0.94     0.80     0.87  \n",
       "32          0.81  0.90     0.77     0.86  \n",
       "33          0.75  0.85     0.69     0.83  \n",
       "34          0.72  0.82     0.64     0.80  \n",
       "35          0.71  0.81     0.61     0.80  \n",
       "36          0.80  0.89     0.76     0.85  \n",
       "37          0.80  0.87     0.76     0.86  \n",
       "38          0.75  0.87     0.69     0.82  \n",
       "39          0.73  0.85     0.66     0.81  \n",
       "40          0.73  0.85     0.66     0.81  \n",
       "41          0.72  0.85     0.65     0.80  \n",
       "42          0.70  0.77     0.62     0.78  \n",
       "43          0.66  0.73     0.54     0.77  \n",
       "44          0.64  0.70     0.50     0.76  \n",
       "45          0.62  0.69     0.46     0.75  \n",
       "46          0.63  0.68     0.49     0.74  \n",
       "47          0.63  0.68     0.49     0.75  \n",
       "48          0.79  0.86     0.75     0.84  \n",
       "49          0.76  0.84     0.70     0.83  \n",
       "50          0.72  0.85     0.65     0.80  \n",
       "51          0.73  0.85     0.67     0.81  \n",
       "52          0.73  0.85     0.66     0.80  \n",
       "53          0.72  0.85     0.65     0.80  \n",
       "54          0.70  0.77     0.62     0.78  \n",
       "55          0.68  0.78     0.57     0.79  \n",
       "56          0.69  0.79     0.58     0.79  \n",
       "57          0.70  0.79     0.61     0.79  \n",
       "58          0.71  0.79     0.62     0.80  \n",
       "59          0.71  0.79     0.63     0.80  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.to_csv('./results/results-binary-LogReg-only.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Process - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=clf.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('Accuracy',accuracy_score(test_labels, predictions))\n",
    "print('F1-score',f1_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
