{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import codecs\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from random import randrange\n",
    "from scipy.sparse import csr_matrix, csc_matrix, hstack, coo_matrix\n",
    "#from gensim.matutils import Scipy2Corpus, corpus2csc\n",
    "#from gensim.models.logentropy_model import LogEntropyModel\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split() if word not in string.punctuation]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['پاکستان کے',\n",
       " 'کے وزیراعظم',\n",
       " 'وزیراعظم عمران',\n",
       " 'عمران خان',\n",
       " 'خان سعودی',\n",
       " 'سعودی عرب',\n",
       " 'عرب کے',\n",
       " 'کے دارالحکومت',\n",
       " 'دارالحکومت ریاض',\n",
       " 'ریاض میں',\n",
       " 'میں ملک',\n",
       " 'ملک میں',\n",
       " 'میں سرمایہ',\n",
       " 'سرمایہ کاری',\n",
       " 'کاری کے',\n",
       " 'کے حوالے',\n",
       " 'حوالے سے',\n",
       " 'سے سالانہ',\n",
       " 'سالانہ کانفرنس',\n",
       " 'کانفرنس میں',\n",
       " 'میں شرکت',\n",
       " 'شرکت کر',\n",
       " 'کر رہے',\n",
       " 'رہے ہیں',\n",
       " 'ہیں حکومت',\n",
       " 'حکومت پاکستان',\n",
       " 'پاکستان کا',\n",
       " 'کا کہنا',\n",
       " 'کہنا ہے',\n",
       " 'ہے کہ',\n",
       " 'کہ سعودی',\n",
       " 'سعودی عرب',\n",
       " 'عرب نے',\n",
       " 'نے پاکستان',\n",
       " 'پاکستان کو',\n",
       " 'کو معاشی',\n",
       " 'معاشی بحران',\n",
       " 'بحران سے',\n",
       " 'سے نمٹنے',\n",
       " 'نمٹنے میں',\n",
       " 'میں مدد',\n",
       " 'مدد کے',\n",
       " 'کے لیے',\n",
       " 'لیے ایک',\n",
       " 'ایک سال',\n",
       " 'سال کے',\n",
       " 'کے لیے',\n",
       " 'لیے تین',\n",
       " 'تین ارب',\n",
       " 'ارب ڈالر',\n",
       " 'ڈالر دینے',\n",
       " 'دینے پر',\n",
       " 'پر اتفاق',\n",
       " 'اتفاق کیا',\n",
       " 'کیا ہے',\n",
       " 'ہے دفترِ',\n",
       " 'دفترِ خارجہ',\n",
       " 'خارجہ کی',\n",
       " 'کی جانب',\n",
       " 'جانب سے',\n",
       " 'سے منگل',\n",
       " 'منگل کی',\n",
       " 'کی شب',\n",
       " 'شب جاری',\n",
       " 'جاری ہونے',\n",
       " 'ہونے والے',\n",
       " 'والے اعلامیے',\n",
       " 'اعلامیے میں',\n",
       " 'میں بتایا',\n",
       " 'بتایا گیا',\n",
       " 'گیا ہے',\n",
       " 'ہے کہ',\n",
       " 'کہ یہ',\n",
       " 'یہ فیصلہ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='پاکستان کے وزیراعظم عمران خان سعودی عرب کے دارالحکومت ریاض میں ملک میں سرمایہ کاری کے حوالے سے سالانہ کانفرنس میں شرکت کر رہے ہیں حکومت پاکستان کا کہنا ہے کہ سعودی عرب نے پاکستان کو معاشی بحران سے نمٹنے میں مدد کے لیے ایک سال کے لیے تین ارب ڈالر دینے پر اتفاق کیا ہے دفترِ خارجہ کی جانب سے منگل کی شب جاری ہونے والے اعلامیے میں بتایا گیا ہے کہ یہ فیصلہ'\n",
    "wordNgrams(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['پ_cng',\n",
       " 'ا_cng',\n",
       " 'ک_cng',\n",
       " 'س_cng',\n",
       " 'ت_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'و_cng',\n",
       " 'ز_cng',\n",
       " 'ی_cng',\n",
       " 'ر_cng',\n",
       " 'ا_cng',\n",
       " 'ع_cng',\n",
       " 'ظ_cng',\n",
       " 'م_cng',\n",
       " ' _cng',\n",
       " 'ع_cng',\n",
       " 'م_cng',\n",
       " 'ر_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'خ_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ع_cng',\n",
       " 'و_cng',\n",
       " 'د_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ع_cng',\n",
       " 'ر_cng',\n",
       " 'ب_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'د_cng',\n",
       " 'ا_cng',\n",
       " 'ر_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " 'ح_cng',\n",
       " 'ک_cng',\n",
       " 'و_cng',\n",
       " 'م_cng',\n",
       " 'ت_cng',\n",
       " ' _cng',\n",
       " 'ر_cng',\n",
       " 'ی_cng',\n",
       " 'ا_cng',\n",
       " 'ض_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ل_cng',\n",
       " 'ک_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ر_cng',\n",
       " 'م_cng',\n",
       " 'ا_cng',\n",
       " 'ی_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ا_cng',\n",
       " 'ر_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ح_cng',\n",
       " 'و_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " 'ف_cng',\n",
       " 'ر_cng',\n",
       " 'ن_cng',\n",
       " 'س_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'ش_cng',\n",
       " 'ر_cng',\n",
       " 'ک_cng',\n",
       " 'ت_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ر_cng',\n",
       " ' _cng',\n",
       " 'ر_cng',\n",
       " 'ہ_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ہ_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'ح_cng',\n",
       " 'ک_cng',\n",
       " 'و_cng',\n",
       " 'م_cng',\n",
       " 'ت_cng',\n",
       " ' _cng',\n",
       " 'پ_cng',\n",
       " 'ا_cng',\n",
       " 'ک_cng',\n",
       " 'س_cng',\n",
       " 'ت_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ا_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ہ_cng',\n",
       " 'ن_cng',\n",
       " 'ا_cng',\n",
       " ' _cng',\n",
       " 'ہ_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ع_cng',\n",
       " 'و_cng',\n",
       " 'د_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ع_cng',\n",
       " 'ر_cng',\n",
       " 'ب_cng',\n",
       " ' _cng',\n",
       " 'ن_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'پ_cng',\n",
       " 'ا_cng',\n",
       " 'ک_cng',\n",
       " 'س_cng',\n",
       " 'ت_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'و_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ع_cng',\n",
       " 'ا_cng',\n",
       " 'ش_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ب_cng',\n",
       " 'ح_cng',\n",
       " 'ر_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ن_cng',\n",
       " 'م_cng',\n",
       " 'ٹ_cng',\n",
       " 'ن_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'د_cng',\n",
       " 'د_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ل_cng',\n",
       " 'ی_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ا_cng',\n",
       " 'ی_cng',\n",
       " 'ک_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ل_cng',\n",
       " 'ی_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ت_cng',\n",
       " 'ی_cng',\n",
       " 'ن_cng',\n",
       " ' _cng',\n",
       " 'ا_cng',\n",
       " 'ر_cng',\n",
       " 'ب_cng',\n",
       " ' _cng',\n",
       " 'ڈ_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " 'ر_cng',\n",
       " ' _cng',\n",
       " 'د_cng',\n",
       " 'ی_cng',\n",
       " 'ن_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'پ_cng',\n",
       " 'ر_cng',\n",
       " ' _cng',\n",
       " 'ا_cng',\n",
       " 'ت_cng',\n",
       " 'ف_cng',\n",
       " 'ا_cng',\n",
       " 'ق_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ی_cng',\n",
       " 'ا_cng',\n",
       " ' _cng',\n",
       " 'ہ_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'د_cng',\n",
       " 'ف_cng',\n",
       " 'ت_cng',\n",
       " 'ر_cng',\n",
       " 'ِ_cng',\n",
       " ' _cng',\n",
       " 'خ_cng',\n",
       " 'ا_cng',\n",
       " 'ر_cng',\n",
       " 'ج_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ج_cng',\n",
       " 'ا_cng',\n",
       " 'ن_cng',\n",
       " 'ب_cng',\n",
       " ' _cng',\n",
       " 'س_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ن_cng',\n",
       " 'گ_cng',\n",
       " 'ل_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ش_cng',\n",
       " 'ب_cng',\n",
       " ' _cng',\n",
       " 'ج_cng',\n",
       " 'ا_cng',\n",
       " 'ر_cng',\n",
       " 'ی_cng',\n",
       " ' _cng',\n",
       " 'ہ_cng',\n",
       " 'و_cng',\n",
       " 'ن_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'و_cng',\n",
       " 'ا_cng',\n",
       " 'ل_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ا_cng',\n",
       " 'ع_cng',\n",
       " 'ل_cng',\n",
       " 'ا_cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'م_cng',\n",
       " 'ی_cng',\n",
       " 'ں_cng',\n",
       " ' _cng',\n",
       " 'ب_cng',\n",
       " 'ت_cng',\n",
       " 'ا_cng',\n",
       " 'ی_cng',\n",
       " 'ا_cng',\n",
       " ' _cng',\n",
       " 'گ_cng',\n",
       " 'ی_cng',\n",
       " 'ا_cng',\n",
       " ' _cng',\n",
       " 'ہ_cng',\n",
       " 'ے_cng',\n",
       " ' _cng',\n",
       " 'ک_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'ی_cng',\n",
       " 'ہ_cng',\n",
       " ' _cng',\n",
       " 'ف_cng',\n",
       " 'ی_cng',\n",
       " 'ص_cng',\n",
       " 'ل_cng',\n",
       " 'ہ_cng']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charNgrams(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts function words n-grams with a pre-loaded dictionary\n",
    "def funcNgrams(text, n):\n",
    "    stop_words = load_diccionario('stop_words.txt')\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r\"(\\n+|\\r+|(\\r\\n)+)\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(funcNgrams(text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diccionario(ruta):\n",
    "    terms = set()#Dictionary of slangs\n",
    "    try:\n",
    "        tmp = open(ruta, \"r\")     \n",
    "        while True :\n",
    "            linea = tmp.readline()                                                                                   \n",
    "            #linea = to_unicode(linea) \n",
    "            if (not linea) or (linea == \"\"):                                                                               \n",
    "                break;                                                                                                      \n",
    "            linea = linea.rstrip()\n",
    "            terms.add(linea.lower())\n",
    "        return (terms)\n",
    "    except IOError as e:\n",
    "        print (\"Error: \"+ruta+\" I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,cn,wn,fn):\n",
    "    text = text.lower()\n",
    "    #text=clean_text(text)\n",
    "    features = []\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in fn:\n",
    "            if n != 0:\n",
    "                features.extend(funcNgrams(text,n))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,cn,wn,fn):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    for (text) in texts:\n",
    "        features=extract_features(text,cn,wn,fn)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function for reading files\n",
    "def read_txt_files(files):\n",
    "    text=[]\n",
    "    topic=[]\n",
    "    for i,file_path in sorted(enumerate(files)):\n",
    "        with open(file_path,'r') as infile:\n",
    "            text.append(infile.read())\n",
    "            #print(file_path)\n",
    "            file_topic=''.join(re.findall('[A-Za-z]',file_path.split('/')[3].split('.')[0]))\n",
    "            #print (file_topic)\n",
    "            topic.append(file_topic)\n",
    "    return text, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the path of real and fake news for training\n",
    "train_path_real='Corpus/Train/Real/'\n",
    "train_path_fake='Corpus/Train/Fake/'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(glob.glob(train_path_real+'*.txt'))\n",
    "fake_news, fake_news_topics = read_txt_files(glob.glob(train_path_fake+'*.txt'))\n",
    "\n",
    "#contatenating real and fake news in one variable for training\n",
    "train_texts = np.concatenate((real_news, fake_news))\n",
    "train_labels = np.concatenate((np.ones(len(real_news)), np.zeros(len(fake_news))))\n",
    "train_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\t Real: 350\n",
      "\t Fake: 288\n"
     ]
    }
   ],
   "source": [
    "print ('Train:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the path of real and fake news for testing\n",
    "test_path_real='Corpus/Test/Real/'\n",
    "test_path_fake='Corpus/Test/Fake/'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(glob.glob(test_path_real+'*.txt'))\n",
    "fake_news, fake_news_topics = read_txt_files(glob.glob(test_path_fake+'*.txt'))\n",
    "\n",
    "#contatenating real and fake news in one variable for testing\n",
    "test_texts = np.concatenate((real_news, fake_news))\n",
    "test_labels = np.concatenate((np.ones(len(real_news)), np.zeros(len(fake_news))))\n",
    "test_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "\t Real: 150\n",
      "\t Fake: 112\n"
     ]
    }
   ],
   "source": [
    "print ('Test:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "\t labels 638\n",
      "\t texts 638\n",
      "\t vocabulary size 15848\n",
      "\t class dictribution Counter({1.0: 350, 0.0: 288})\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "cnvalues=[0] #character n-grams\n",
    "wnvalues=[1] # word n-grams\n",
    "fnvalues=[0] # function words n-grams\n",
    "\n",
    "print('Extracting features')\n",
    "train_features, dicOfFeatures = process_texts(train_texts,cnvalues,wnvalues,fnvalues)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, min_df=2, tokenizer=lambda x: x.split('&%$')) #--> we can change this\n",
    "train_data = vectorizer.fit_transform(train_features)\n",
    "train_data = train_data.astype(float)\n",
    "print('\\t', 'labels', len(train_labels))\n",
    "print('\\t', 'texts', len(train_texts))\n",
    "print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "print('\\t', 'class dictribution',Counter(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638, 4099)\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "X=train_data\n",
    "values=np.array(X.sum(axis=0)).ravel()\n",
    "thresholdMask=(values >= N)*1\n",
    "indices_zero = list(np.nonzero(thresholdMask == 0)[0])\n",
    "all_cols = np.arange(X.shape[1])\n",
    "cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "train_data = X[:, cols_to_keep]\n",
    "#####\n",
    "\n",
    "print(train_data.shape)\n",
    "    \n",
    "scaled_train_data=train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_weight = tf\n"
     ]
    }
   ],
   "source": [
    "feature_weight='tf' # possible values: binary, logent, tfidf, norm, relat\n",
    "\n",
    "if feature_weight == 'binary':\n",
    "    scaled_train_data = preprocessing.Binarizer().fit_transform(scaled_train_data)\n",
    "\n",
    "elif feature_weight == 'logent':\n",
    "    Xc = Scipy2Corpus(scaled_train_data)\n",
    "    log_ent = LogEntropyModel(Xc)\n",
    "    X = log_ent[Xc]\n",
    "    X = corpus2csc(X)\n",
    "    scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "\n",
    "elif feature_weight == 'tfidf':\n",
    "    transformer = TfidfTransformer()\n",
    "    scaled_train_data = transformer.fit_transform(scaled_train_data)\n",
    "\n",
    "elif feature_weight=='norm':\n",
    "    #scaled_train_data = preprocessing.normalize(scaled_train_data, norm='l2')\n",
    "    #Scaling data\n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "    \n",
    "elif feature_weight=='relat':\n",
    "    s = scaled_train_data.sum(axis = 1)\n",
    "    scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/s))\n",
    "\n",
    "else:\n",
    "    print (\"feature_weight = tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function\n",
    "originalclass=[]\n",
    "predictedclass=[]\n",
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return f1_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Process - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.69      0.70       576\n",
      "        1.0       0.75      0.77      0.76       700\n",
      "\n",
      "avg / total       0.73      0.74      0.73      1276\n",
      "\n",
      "10-Fold Cross-validation Linear SVC 0.7601637727325983\n",
      "10-Fold Cross-validation Logistic Regression 0.740239963534213\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.671410188218199\n",
      "10-Fold Cross-validation Bernoulli Naive Bayes 0.6358633350349778\n"
     ]
    }
   ],
   "source": [
    "print('Training Classifier')\n",
    "    \n",
    "# Applying classification algorithms\n",
    "clf=LinearSVC(C=0.01,class_weight='balanced', random_state=85)\n",
    "clfSVC=SVC(C=0.01, kernel='linear',class_weight='balanced')\n",
    "clfMnb=MultinomialNB()\n",
    "clfBnb=BernoulliNB()\n",
    "clfLG=LogisticRegression(solver='lbfgs', tol=0.001, C=0.01,class_weight='balanced')\n",
    "\n",
    "nested_score = cross_val_score(clf, X=scaled_train_data, y=train_labels, cv=10, scoring=make_scorer(classification_report_with_f1_score))\n",
    "#cvScoreLinearSVC=cross_val_score(clf, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "print('10-Fold Cross-validation Linear SVC',nested_score.mean())\n",
    "\n",
    "cvScoreLG=cross_val_score(clfLG, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "\n",
    "cvScoreMnb=cross_val_score(clfMnb, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "\n",
    "cvScoreBnb=cross_val_score(clfBnb, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "print('10-Fold Cross-validation Bernoulli Naive Bayes',cvScoreBnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
